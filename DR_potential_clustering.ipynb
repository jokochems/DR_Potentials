{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical demand response potential clustering\n",
    "\n",
    "**Puporse and background**: This notebook serves for clustering the technical demand response potentials \n",
    "which were collected in a previous meta-analysis (Kochems 2020). This clustering routine is necessary since depicting all \n",
    "different units in the electricity market model would be computationally too expensive.\n",
    "\n",
    "## Method applied\n",
    "In the following, a brief description of the method applied is given for the sake of replicability.\n",
    "\n",
    "### Filtering of demand response categories\n",
    "The term \"demand response categories\" is introduced to describe the heterogeneous potential segmentation \n",
    "routines used in the publications evaluated within the meta-analysis. These demand response categories include \n",
    "processes, applications as well as entrire branches and mixtures of these categories.\n",
    "* In the first step, data for entire branches is filtered out since in most cases there are too few data points and \n",
    "further information on availability is lacking. Thus, only data for processes and applications remain.\n",
    "* In addition to that, categories covering different branches / appliances are filtered out since these would \n",
    "cause redundancies to publications analyzing the appliances in detail.\n",
    "\n",
    "### Prepare and manipulate data for further usage (clustering / modeling)\n",
    "In the second step, the data is manipulated for further usage in the clustering process as well as for the ensuing power market model analysis. This in turn consists of a few procedures:\n",
    "* The data is combined to an overall data set and missing values are interpolated using the median value per sector. The parameters are filtered in order only to include those which are needed in demand response modeling in a power market model.\n",
    "* A pairwhise correlation analysis using pearson's correlation coefficient is carried out in order to identify which parameters can be expressed through other ones since they show a high correlation. \n",
    "* Data is interpolated in order to remove inplausibilities. The data for the status quo is kept. The values for 2030 and 2050 are used to define trends in potential development. A linear interpolation is made in between. As far as the cost values are concerned, the values for the status quo as well as the minimum value are kept. The minimum value is assigned to 2050 and a linear interpolation is made in between.\n",
    "> _Note: While the interpolation solution is preferable in terms of simplicity, it lacks some consistency: on the one hand in terms of a development of positive and negative potentials in line, on the other hand as for general trends within the sectors (such as declining industry production in the long run). This should be replaced by a more profound approach._\n",
    "\n",
    "### Clustering of demand response categories\n",
    "A clustering of demand response categories is carried out in the second step. \n",
    "A k-means clustering approach is used (as an alternative, it is possible to choose agglomerative clustering using ward linkage).<br>\n",
    "Demand response categories are clustered using the (median values of the) following parameters (see also Steurer 2017, p. 83):\n",
    "* shifting duration\n",
    "* positive interference duration (shedding duration),\n",
    "* variable costs,\n",
    "* fixed costs and\n",
    "* specific investments.\n",
    "\n",
    "Some further aspects are worth mentioning:\n",
    "* Negative interference duration is not taken into account because some processes are only eligible for load shedding and hence don't have a negative interference duration. In addition to that, a strong correlation between positive and negative interference duration has been detected.\n",
    "* The clustering does not need to take into account the lower, middle and upper value for each parameter. A strong correlation between the values was determined which is why only the median values are used for the clustering. \n",
    "* Furthermore, the clustering is only carried out for the status quo and does not take development projections into account.\n",
    "* The distinction between different sectors is kept. Some heating and cooling applications for tcs and households are combined since they comprise basically the same technology and creating identical clusters would not make much sense.\n",
    "* For the aggregation of demand response parameters after clustering, a weighting by the available shifting resp. shedding capacity is carried out.\n",
    "\n",
    "### Determination of availability\n",
    "Since demand response potentials are time-dependent, availability has to be taken into account.<br>\n",
    "For the analysis, the individual availability time series in hourly resolution of the original \n",
    "demand response categories are put together by calculating capacity weighted averages for the identified \n",
    "demand response clusters.\n",
    "\n",
    "The availability time series are put together based on literature assumptions:\n",
    "* The largest amount of the availability time series for individual demand response categories were created within three bachelor theses based on literature assumptions. They were put together in a separate notebook. The data output of this notebook in turn is read in here to form availability time series of demand response clusters.\n",
    "* Some processes haven't been analyzed in the bachelor theses resp. the literature. For these, either existing availability time series of very similar demand response categories are assigned or own assumptions are made.\n",
    "> _Note: These own assumptions for now are (\"best guess\") dummy values and should be replaced by more profound ones._\n",
    "***\n",
    "\n",
    "__*Important remaining TODOs:*__\n",
    "* __*Revise parameters and potential development by including trend extrapolation / assumptions*__\n",
    "* __*Revise proxies for availability time series*__\n",
    "* __*Include the distinction between shifting and shedding &rarr; should be a criterion for separate cluster assignment*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package imports\n",
    "* Standard imports: scikit-learn (sklearn) is used for the clustering since it has built-in clustering routines, such as K-means\n",
    "* User-defined functions:\n",
    "    * *create_parameter_combinations*: Combines parameter names with measures of central tendency\n",
    "    * *group_potential*: Does a grouping of the clusters determined using given aggregation functions per parameter\n",
    "    * *wtrie_multiple_sheets*: Used to write multiple DataFrames as sheets at once into an Excel workbook\n",
    "    * *map_column_names*: Maps column names of availability time series to the potential data column names.\n",
    "    * *determine_missing_columns*: Lists the columns for which availability time series information is lacking and assumptions are needed.\n",
    "    * *get_top_abs_correlations*: Determines the strongest correlation within a given correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.stats import linregress\n",
    "from IPython.core.display import display\n",
    "\n",
    "from potential_clustering_funcs import (\n",
    "    create_parameter_combinations, group_potential, write_multiple_sheets,\n",
    "    map_column_names, determine_missing_cols, assign_availability_remaining, get_top_abs_correlations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter settings\n",
    "Set path folder and filenames for reading in data.\n",
    "\n",
    "Further parameters for controlling the workflow:\n",
    "* *skip_dev*: If True, skip cells that are currently under development to keep overall functionality of notebook\n",
    "* *write_outputs*: If True, outputs, i.e. demand response parameterizations resulting from the meta-analysis will be written into Excel workbooks\n",
    "* *join_duplicates*: If True, demand response categories occuring in both, household and tcs sector will be combined together.\n",
    "* *write_categories*: If True, the remaining categories will be written to Excel in order to match them with the\n",
    "availability data.\n",
    "* *adjust_potentials*: If True, availability time series information will be used to adjust the potential information. (I. e. if at max. 0.8 is reached, max potential will be set to 0.8 * max_potential)\n",
    "\n",
    "Parameters for controlling the clustering routines:\n",
    "* *cols*: Determine, which columns of the stats DataFrame shall be used for demand response parameterization\n",
    "    * Usually lower and upper quartile as well as median are used.\n",
    "> Note: This eliminates extreme values which are taken into account when min and max are used instead.<br>\n",
    "The upper quartile of overall potential will be interpreted as maximum available potential. A possible alternative would be to use min and max values to depict extremes. &rarr; Advantage: would be more compelling. Drawback: Data lack for potential max.\n",
    "* *cluster_parameters*: Determine, which demand response parameters to use for the clustering process. \n",
    "By default, these are the shifting duration, positive interference duration, variable costs as well as fixed costs.\n",
    "* *cluster_algo*: The clustering algorithmn to be used (\"KMeans\" or \"ward\")\n",
    "* *share_clusters*: Decide, how strong the original data will be reduced by giving a percentage of the original length. \n",
    "The cluster number is determined by the next higher integer. (Only applicable for k-means)\n",
    "* *distance_threshold*: Decide, what distance threshold shall be set for the hierarchical clustering using ward \n",
    "linkage, i.e., when the algorithm should terminate.\n",
    "* *print_clusters*: If True, prints out the clusters created (DataFrames)\n",
    "* *use_ava_ts_for_profiles*: If True, availability time series in positive direction will be directly used to derive load profiles for the demand response categories resp. clusters, else profiles from the demand regio disaggregator will be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Set path folder(s) and filename(s) for reading in / writing data\n",
    "path_folder_in = \"./\"\n",
    "path_folder_stats = \"./out/stats/\"\n",
    "path_folder_availability = \"./out/availability/\"\n",
    "path_folder_parameterization = \"./out/parameterization/\"\n",
    "filename_in = \"Potenziale_Lastmanagement.xlsx\"\n",
    "filename_availability_in = \"availability_timeseries.xlsx\"\n",
    "filename_out = \"parameterization\"\n",
    "filename_corr_out = \"correlation\"\n",
    "filename_availability_out = \"availability_timeseries_clusters.csv\"\n",
    "filename_load_profiles_out = \"load_profile_timeseries_clusters\"\n",
    "\n",
    "# Set further parameters for controlling the workflow\n",
    "skip_dev = True\n",
    "write_outputs = True\n",
    "join_duplicates = True\n",
    "write_categories = True\n",
    "adjust_potentials = True\n",
    "\n",
    "# Determine clustering approach\n",
    "cols = [\"25%\", \"50%\", \"75%\"]\n",
    "cluster_parameters = [\"shifting_duration\", \"interference_duration_pos\", \n",
    "                      \"variable_costs\", \"fixed_costs\", \"specific_investments\"]\n",
    "# Correlation analysis: optionally add potential_pos_overall (for all potential metrics)\n",
    "\n",
    "cluster_algo = \"KMeans\"\n",
    "share_clusters = 0.1\n",
    "distance_threshold = 1000\n",
    "print_clusters = True\n",
    "use_ava_ts_for_profiles = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in and filter data\n",
    "* Read in the categories data and filter out branches as well as power-to-X-technologies which won't be considered anymore.\n",
    "* Read in the stats information on the demand response parameters from the previous meta-analysis.\n",
    "\n",
    "> _NOTE: Data that is read in here could also be obtained directly from the DataFrames of the potential evaluation notebook. A separate notebook is used here only for the sake of shortness and readability._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in and filter demand response categories\n",
    "* Read in demand response categories\n",
    "* Drop entire branches as well as categories conflicting with others or outside of scope (Power-to-X other than Power-to-Heat for space heating).\n",
    "* Show the original number of categories and print the remaining number after filtering (without duplicates since categories may be used within different sectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original categories:\t99\n",
      "Number of remaining categories:\t38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Chlor-Alkali-Elektrolyse',\n",
       " 'Luftzerlegung',\n",
       " 'Primäraluminiumelektrolyse',\n",
       " 'Kupfer- und Zinkherstellung (Elektrolyse)',\n",
       " 'Holz- und Zellstoffherstellung',\n",
       " 'Altpapierrecycling (Pulper)',\n",
       " 'Papiermaschinen',\n",
       " 'Papierveredelung (Streichmaschinen und Kalander)',\n",
       " 'Elektrostahlherstellung (Lichtbogenofen)',\n",
       " 'Zementherstellung',\n",
       " 'Kühlung (Lebensmittelindustrie)',\n",
       " 'Belüftung',\n",
       " 'Kühlhäuser',\n",
       " 'Prozesskälte',\n",
       " 'Klimakälte',\n",
       " 'Warmwasserbereitstellung',\n",
       " 'Nachtspeicherheizungen',\n",
       " 'Notstromaggregate, Back-Up-Server und Mobilfunkstationen',\n",
       " 'Waschmaschinen',\n",
       " 'Wäschetrockner',\n",
       " 'Geschirrspüler',\n",
       " 'Kühlschränke',\n",
       " 'Gefrierschränke und -truhen',\n",
       " 'Wärmepumpen',\n",
       " 'Heizungsumwälzpumpen',\n",
       " 'Prozesswärme',\n",
       " 'Druckluftanwendungen',\n",
       " 'Abwasserbehandlung',\n",
       " 'Kühl- und Gefrierkombinationen',\n",
       " 'Calciumcarbid-Herstellung (Lichtbogenofen)',\n",
       " 'Pumpenanwendungen in der Wasserversorgung',\n",
       " 'Prozesskälte Handel',\n",
       " 'Beleuchtung',\n",
       " 'Pumpenanwendungen',\n",
       " 'Gießereien (Induktionsofen)',\n",
       " 'Zerkleinerer',\n",
       " 'Elektrodenheizkessel',\n",
       " 'Elektrische Öfen']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = pd.read_excel(path_folder_in+filename_in, sheet_name=\"Kategorien_neu\", index_col=0)\n",
    "\n",
    "print(\"Number of original categories:\\t{}\".format(categories.shape[0]))\n",
    "\n",
    "categories = categories[\n",
    "    categories[\"Nutzung?\"] == 1 \n",
    "    & ~categories[\"Einstufung\"].isin([\"Branche\", \"Power-to-X\"])\n",
    "]\n",
    "\n",
    "categories = categories.drop_duplicates(subset=\"Prozesskategorie\")\n",
    "categories = categories.set_index([\"Prozesskategorie\"], drop=True)\n",
    "\n",
    "print(\"Number of remaining categories:\\t{}\".format(categories.shape[0]))\n",
    "\n",
    "# Show the remaining demand response categories which are evaluated\n",
    "list(categories.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in demand response parameters data\n",
    "* Assign each demand response parameter an aggregation function to be used after clustering (sum or mean).\n",
    "* Determine for which parameters to swap the order of preference.\n",
    "> _Note: This is necessary, because in some cases minimum values are needed for an optimistic \n",
    "demand response projection and maximum for a pessimistic one, e.g. for minimum load factor. <br>\n",
    "Hence, for these parameters, min is exchanged for max etc._\n",
    "* Read in the data and store it in a dictionary indexed by demand response parameter. Hereby, a separation is necessary:\n",
    "    * For the clustering process, certain appliances for the household and tcs sector should be combined, e.g. heat pumps. A new sector \"tcs+hoho\" is introduced for this purpose and the respective values are combined.\n",
    "    * For calculating availabilities, a (re-)separation is necessary. This is because availability time series of the appliances may (slightly) differ among the sectors. &rarr; Herefore, a second dict is introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Assgin each parameter the aggregation function to be used\n",
    "parameters_agg_dict = {\n",
    "    \"activation_duration\": \"mean\", \n",
    "    \"ave_load\": \"mean\", \n",
    "    \"fixed_costs\": \"mean\", \n",
    "    \"installed_cap\": \"sum\",\n",
    "    \"interference_duration_neg\": \"mean\", \n",
    "    \"interference_duration_pos\": \"mean\",\n",
    "    \"interference_duration_pos_shed\": \"mean\",\n",
    "    \"max_load\": \"mean\", \n",
    "    \"maximum_activations_year\": \"mean\", \n",
    "    \"maximum_activations_year_shed\": \"mean\",\n",
    "    \"min_load\": \"mean\", \n",
    "    \"potential_neg_overall\": \"sum\",\n",
    "    \"potential_pos_overall\": \"sum\",\n",
    "    \"potential_pos_overall_shed\": \"sum\",\n",
    "    \"regeneration_duration\": \"mean\", \n",
    "    \"shiftable_share\": \"mean\", \n",
    "    \"shifting_duration\": \"mean\",\n",
    "    \"specific_investments\": \"mean\", \n",
    "    \"variable_costs\": \"mean\",\n",
    "    \"variable_costs_shed\": \"mean\"\n",
    "}\n",
    "\n",
    "# Determine for each parameter whether or not to swap values\n",
    "parameters_swap_dict = {\n",
    "    \"activation_duration\": True, \n",
    "    \"ave_load\": True, \n",
    "    \"fixed_costs\": True, \n",
    "    \"installed_cap\": False,\n",
    "    \"interference_duration_neg\": False, \n",
    "    \"interference_duration_pos\": False,\n",
    "    \"interference_duration_pos_shed\": False,\n",
    "    \"max_load\": False, \n",
    "    \"maximum_activations_year\": False, \n",
    "    \"maximum_activations_year_shed\": False,\n",
    "    \"min_load\": True, \n",
    "    \"potential_neg_overall\": True,\n",
    "    \"potential_pos_overall\": False,\n",
    "    \"potential_pos_overall_shed\": False,\n",
    "    \"regeneration_duration\": True, \n",
    "    \"shiftable_share\": False, \n",
    "    \"shifting_duration\": False,\n",
    "    \"specific_investments\": True, \n",
    "    \"variable_costs\": True,\n",
    "    \"variable_costs_shed\": True  \n",
    "}\n",
    "\n",
    "# Map columns for swapping\n",
    "swap_cols = {\n",
    "    \"min\": \"max\",\n",
    "    \"5%\": \"95%\",\n",
    "    \"10%\": \"90%\",\n",
    "    \"25%\": \"75%\"\n",
    "}\n",
    "\n",
    "sectors = [\"ind\", \"tcs\", \"hoho\"]\n",
    "\n",
    "years = [\"SQ\", \"2020\", \"2025\", \"2030\", \"2035\", \"2040\", \"2045\", \"2050\"]\n",
    "\n",
    "to_join = [\"Nachtspeicherheizungen\", \"Warmwasserbereitstellung\", \"Wärmepumpen\", \"Klimakälte\"]\n",
    "to_drop = {\"Prozesskälte\": \"hoho\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _NOTE: While the fundamental routine here is quite okay and fast enough, there is room for improvement in the aggregation routine for the duplicates DataFrame, i.e. the DataFrame containing data for tcs+hoho which is then aggregated. Fo a compelling approach, a capacity weighted average should be calaculated using overall positive potential as capacity information. In order to do so, capacity information has to be added to the individual data sets._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Chlor-Alkali-Elektrolyse', 'Luftzerlegung',\n",
       "       'Primäraluminiumelektrolyse',\n",
       "       'Kupfer- und Zinkherstellung (Elektrolyse)',\n",
       "       'Holz- und Zellstoffherstellung', 'Altpapierrecycling (Pulper)',\n",
       "       'Papiermaschinen', 'Papierveredelung (Streichmaschinen und Kalander)',\n",
       "       'Elektrostahlherstellung (Lichtbogenofen)', 'Zementherstellung',\n",
       "       'Kühlung (Lebensmittelindustrie)', 'Belüftung', 'Kühlhäuser',\n",
       "       'Prozesskälte', 'Klimakälte', 'Warmwasserbereitstellung',\n",
       "       'Nachtspeicherheizungen',\n",
       "       'Notstromaggregate, Back-Up-Server und Mobilfunkstationen',\n",
       "       'Waschmaschinen', 'Wäschetrockner', 'Geschirrspüler', 'Kühlschränke',\n",
       "       'Gefrierschränke und -truhen', 'Wärmepumpen', 'Heizungsumwälzpumpen',\n",
       "       'Prozesswärme', 'Druckluftanwendungen', 'Abwasserbehandlung',\n",
       "       'Kühl- und Gefrierkombinationen',\n",
       "       'Calciumcarbid-Herstellung (Lichtbogenofen)',\n",
       "       'Pumpenanwendungen in der Wasserversorgung', 'Prozesskälte Handel',\n",
       "       'Beleuchtung', 'Pumpenanwendungen', 'Gießereien (Induktionsofen)',\n",
       "       'Zerkleinerer', 'Elektrodenheizkessel', 'Elektrische Öfen'],\n",
       "      dtype='object', name='Prozesskategorie')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental: Update / facilitate the processing routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a common data basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall number of params (sectors, years, params): 480\n",
      "Number of params not eligible for evaluation: 114\n"
     ]
    }
   ],
   "source": [
    "# Put everything into one common pandas.DataFrame\n",
    "all_params_df = pd.DataFrame()\n",
    "\n",
    "# Count the number of params for which no data is available\n",
    "count_ignored = 0\n",
    "\n",
    "for parameter, swap_param in parameters_swap_dict.items():\n",
    "    for year in years:\n",
    "        for sector in sectors:\n",
    "            try:\n",
    "                new_df = pd.read_csv(\n",
    "                    path_folder_stats+parameter+\"_\"+sector+\"_stats_\" + year + \".csv\",\n",
    "                    sep=\";\",\n",
    "                    index_col=0\n",
    "                ).T\n",
    "                new_df.index.name = \"Prozesskategorie\"\n",
    "\n",
    "                # Change the order of appearance if swap is needed for parameter\n",
    "                if swap_param:\n",
    "                    for k, v in swap_cols.items():\n",
    "                        new_df[k + \"_copy\"] = new_df[k]\n",
    "                        new_df[v + \"_copy\"] = new_df[v]\n",
    "                        new_df[k] = new_df[v + \"_copy\"]\n",
    "                        new_df[v] = new_df[k + \"_copy\"]\n",
    "                        new_df.drop(columns=[k + \"_copy\", v + \"_copy\"], inplace=True)\n",
    "                    \n",
    "                new_df[\"parameter\"] = parameter\n",
    "                new_df[\"sector\"] = sector\n",
    "                new_df[\"year\"] = year\n",
    "                all_params_df = pd.concat([all_params_df, new_df], sort=\"False\")\n",
    "            except:\n",
    "                count_ignored += 1\n",
    "                continue\n",
    "\n",
    "print(\"Overall number of params (sectors, years, params): \" + str(len(parameters_agg_dict) * (len(sectors)) * len(years)))\n",
    "print(\"Number of params not eligible for evaluation: \" + str(count_ignored))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koch_j0\\.conda\\envs\\diss3\\lib\\site-packages\\pandas\\core\\frame.py:4167: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "# Filter out the categories to be used and drop certain ones for dedicated sector only\n",
    "all_params_df = all_params_df.loc[all_params_df.index.isin(categories.index)]\n",
    "all_params_df.set_index(\"sector\", append=True, inplace=True)\n",
    "all_params_df.drop(index={(k, v) for k, v in to_drop.items()}, inplace=True)\n",
    "all_params_df.reset_index(level=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine duplicates for tcs & hoho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend index and filter for duplicate values\n",
    "all_params_df.set_index([\"year\", \"parameter\"], append=True, inplace=True)\n",
    "filter_duplicates = (\n",
    "    all_params_df.index.duplicated(keep=False)\n",
    "    & all_params_df.sector.isin([\"hoho\", \"tcs\"])\n",
    ")\n",
    "\n",
    "# Get duplicates within duplicates (i.e. filter out the values where \n",
    "# the same demand response categories occur in tcs and industry sector)\n",
    "duplicates_df = all_params_df.loc[filter_duplicates]\n",
    "duplicates_df = duplicates_df[duplicates_df.index.duplicated(keep=False)]\n",
    "\n",
    "# Filter out the remaining duplicate values from original DataFrame\n",
    "keys = list(duplicates_df.columns.values)\n",
    "i1 = all_params_df.set_index(keys).index\n",
    "i2 = duplicates_df.set_index(keys).index\n",
    "no_duplicates_df = all_params_df.loc[~i1.isin(i2)]\n",
    "\n",
    "# Assign certain demand response categories to a combined tcs & hoho sector (\"tcs+hoho\")\n",
    "for el in to_join:\n",
    "    if el in no_duplicates_df.index:\n",
    "        no_duplicates_df.loc[:,\"help_sector\"] = np.where(\n",
    "            no_duplicates_df.loc[:,\"sector\"].values == \"ind\", \n",
    "            \"ind\", \n",
    "            \"tcs+hoho\"\n",
    "        )\n",
    "        no_duplicates_df.loc[el, \"sector\"] = no_duplicates_df.loc[el, \"help_sector\"].values\n",
    "        no_duplicates_df = no_duplicates_df.drop([\"help_sector\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop index for grouping\n",
    "duplicates_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "# Introduce DataFrame for grouping duplicates (sector \"tcs+hoho\")\n",
    "grouped_duplicates_df = pd.DataFrame()\n",
    "\n",
    "for parameter, param_agg_rule in parameters_agg_dict.items():\n",
    "    param_duplicates_df = duplicates_df.loc[duplicates_df[\"parameter\"] == parameter]\n",
    "    if not param_duplicates_df.empty:\n",
    "        param_duplicates_df = param_duplicates_df.groupby([\"Prozesskategorie\", \"year\", \"parameter\"]).agg({\n",
    "            \"count\": \"sum\",\n",
    "            \"min\": param_agg_rule,\n",
    "            \"5%\": param_agg_rule,\n",
    "            \"10%\": param_agg_rule,\n",
    "            \"25%\": param_agg_rule,\n",
    "            \"50%\": param_agg_rule,\n",
    "            \"mean\": param_agg_rule,\n",
    "            \"std\": param_agg_rule,\n",
    "            \"75%\": param_agg_rule,\n",
    "            \"90%\": param_agg_rule,\n",
    "            \"95%\": param_agg_rule,\n",
    "            \"max\": param_agg_rule,\n",
    "        })\n",
    "        grouped_duplicates_df = pd.concat([grouped_duplicates_df, param_duplicates_df])\n",
    "        \n",
    "grouped_duplicates_df[\"sector\"] = \"tcs+hoho\"\n",
    "\n",
    "# Combine the non-duplicated data with the consolidated tcs+hoho data\n",
    "all_params_df = pd.concat([no_duplicates_df, grouped_duplicates_df], sort=False)\n",
    "\n",
    "sectors.append(\"tcs+hoho\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sector</th>\n",
       "      <th>10%</th>\n",
       "      <th>25%</th>\n",
       "      <th>5%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>90%</th>\n",
       "      <th>95%</th>\n",
       "      <th>count</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prozesskategorie</th>\n",
       "      <th>year</th>\n",
       "      <th>parameter</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Altpapierrecycling (Pulper)</th>\n",
       "      <th>SQ</th>\n",
       "      <th>activation_duration</th>\n",
       "      <td>ind</td>\n",
       "      <td>1.885833</td>\n",
       "      <td>0.703750</td>\n",
       "      <td>2.442917</td>\n",
       "      <td>0.374583</td>\n",
       "      <td>0.124792</td>\n",
       "      <td>0.045833</td>\n",
       "      <td>0.027083</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.768750</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.128602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Calciumcarbid-Herstellung (Lichtbogenofen)</th>\n",
       "      <th>SQ</th>\n",
       "      <th>activation_duration</th>\n",
       "      <td>ind</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.074536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chlor-Alkali-Elektrolyse</th>\n",
       "      <th>SQ</th>\n",
       "      <th>activation_duration</th>\n",
       "      <td>ind</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.222143</td>\n",
       "      <td>0.055205</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.251187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.310194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Elektrostahlherstellung (Lichtbogenofen)</th>\n",
       "      <th>SQ</th>\n",
       "      <th>activation_duration</th>\n",
       "      <td>ind</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.821969</td>\n",
       "      <td>2.266667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.584455</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.924036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gießereien (Induktionsofen)</th>\n",
       "      <th>SQ</th>\n",
       "      <th>activation_duration</th>\n",
       "      <td>ind</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.074536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Warmwasserbereitstellung</th>\n",
       "      <th>2030</th>\n",
       "      <th>shifting_duration</th>\n",
       "      <td>tcs+hoho</td>\n",
       "      <td>4.910000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.320000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.600000</td>\n",
       "      <td>17.800000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>11.326923</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.863322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2050</th>\n",
       "      <th>shifting_duration</th>\n",
       "      <td>tcs+hoho</td>\n",
       "      <td>4.440000</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.600000</td>\n",
       "      <td>17.800000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>10.442308</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>6.200191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SQ</th>\n",
       "      <th>shifting_duration</th>\n",
       "      <td>tcs+hoho</td>\n",
       "      <td>2.087500</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.738971</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>7.580785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Klimakälte</th>\n",
       "      <th>2020</th>\n",
       "      <th>specific_investments</th>\n",
       "      <td>tcs+hoho</td>\n",
       "      <td>132.357644</td>\n",
       "      <td>105.362995</td>\n",
       "      <td>141.355861</td>\n",
       "      <td>81.589853</td>\n",
       "      <td>73.816154</td>\n",
       "      <td>69.151935</td>\n",
       "      <td>67.597195</td>\n",
       "      <td>7.0</td>\n",
       "      <td>66.042455</td>\n",
       "      <td>95.792472</td>\n",
       "      <td>150.354077</td>\n",
       "      <td>39.602507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SQ</th>\n",
       "      <th>specific_investments</th>\n",
       "      <td>tcs+hoho</td>\n",
       "      <td>606.702477</td>\n",
       "      <td>422.131192</td>\n",
       "      <td>668.226238</td>\n",
       "      <td>96.399853</td>\n",
       "      <td>79.295725</td>\n",
       "      <td>74.123643</td>\n",
       "      <td>72.399616</td>\n",
       "      <td>9.0</td>\n",
       "      <td>70.675588</td>\n",
       "      <td>279.650472</td>\n",
       "      <td>729.750000</td>\n",
       "      <td>295.387668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3024 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        sector  \\\n",
       "Prozesskategorie                           year parameter                        \n",
       "Altpapierrecycling (Pulper)                SQ   activation_duration        ind   \n",
       "Calciumcarbid-Herstellung (Lichtbogenofen) SQ   activation_duration        ind   \n",
       "Chlor-Alkali-Elektrolyse                   SQ   activation_duration        ind   \n",
       "Elektrostahlherstellung (Lichtbogenofen)   SQ   activation_duration        ind   \n",
       "Gießereien (Induktionsofen)                SQ   activation_duration        ind   \n",
       "...                                                                        ...   \n",
       "Warmwasserbereitstellung                   2030 shifting_duration     tcs+hoho   \n",
       "                                           2050 shifting_duration     tcs+hoho   \n",
       "                                           SQ   shifting_duration     tcs+hoho   \n",
       "Klimakälte                                 2020 specific_investments  tcs+hoho   \n",
       "                                           SQ   specific_investments  tcs+hoho   \n",
       "\n",
       "                                                                             10%  \\\n",
       "Prozesskategorie                           year parameter                          \n",
       "Altpapierrecycling (Pulper)                SQ   activation_duration     1.885833   \n",
       "Calciumcarbid-Herstellung (Lichtbogenofen) SQ   activation_duration     0.250000   \n",
       "Chlor-Alkali-Elektrolyse                   SQ   activation_duration     0.625000   \n",
       "Elektrostahlherstellung (Lichtbogenofen)   SQ   activation_duration     1.600000   \n",
       "Gießereien (Induktionsofen)                SQ   activation_duration     0.250000   \n",
       "...                                                                          ...   \n",
       "Warmwasserbereitstellung                   2030 shifting_duration       4.910000   \n",
       "                                           2050 shifting_duration       4.440000   \n",
       "                                           SQ   shifting_duration       2.087500   \n",
       "Klimakälte                                 2020 specific_investments  132.357644   \n",
       "                                           SQ   specific_investments  606.702477   \n",
       "\n",
       "                                                                             25%  \\\n",
       "Prozesskategorie                           year parameter                          \n",
       "Altpapierrecycling (Pulper)                SQ   activation_duration     0.703750   \n",
       "Calciumcarbid-Herstellung (Lichtbogenofen) SQ   activation_duration     0.229167   \n",
       "Chlor-Alkali-Elektrolyse                   SQ   activation_duration     0.250000   \n",
       "Elektrostahlherstellung (Lichtbogenofen)   SQ   activation_duration     0.821969   \n",
       "Gießereien (Induktionsofen)                SQ   activation_duration     0.229167   \n",
       "...                                                                          ...   \n",
       "Warmwasserbereitstellung                   2030 shifting_duration       7.000000   \n",
       "                                           2050 shifting_duration       6.750000   \n",
       "                                           SQ   shifting_duration       5.000000   \n",
       "Klimakälte                                 2020 specific_investments  105.362995   \n",
       "                                           SQ   specific_investments  422.131192   \n",
       "\n",
       "                                                                              5%  \\\n",
       "Prozesskategorie                           year parameter                          \n",
       "Altpapierrecycling (Pulper)                SQ   activation_duration     2.442917   \n",
       "Calciumcarbid-Herstellung (Lichtbogenofen) SQ   activation_duration     0.250000   \n",
       "Chlor-Alkali-Elektrolyse                   SQ   activation_duration     1.000000   \n",
       "Elektrostahlherstellung (Lichtbogenofen)   SQ   activation_duration     2.266667   \n",
       "Gießereien (Induktionsofen)                SQ   activation_duration     0.250000   \n",
       "...                                                                          ...   \n",
       "Warmwasserbereitstellung                   2030 shifting_duration       4.320000   \n",
       "                                           2050 shifting_duration       3.600000   \n",
       "                                           SQ   shifting_duration       0.981250   \n",
       "Klimakälte                                 2020 specific_investments  141.355861   \n",
       "                                           SQ   specific_investments  668.226238   \n",
       "\n",
       "                                                                            50%  \\\n",
       "Prozesskategorie                           year parameter                         \n",
       "Altpapierrecycling (Pulper)                SQ   activation_duration    0.374583   \n",
       "Calciumcarbid-Herstellung (Lichtbogenofen) SQ   activation_duration    0.166667   \n",
       "Chlor-Alkali-Elektrolyse                   SQ   activation_duration    0.222143   \n",
       "Elektrostahlherstellung (Lichtbogenofen)   SQ   activation_duration    0.166667   \n",
       "Gießereien (Induktionsofen)                SQ   activation_duration    0.166667   \n",
       "...                                                                         ...   \n",
       "Warmwasserbereitstellung                   2030 shifting_duration     10.000000   \n",
       "                                           2050 shifting_duration     10.000000   \n",
       "                                           SQ   shifting_duration      9.000000   \n",
       "Klimakälte                                 2020 specific_investments  81.589853   \n",
       "                                           SQ   specific_investments  96.399853   \n",
       "\n",
       "                                                                            75%  \\\n",
       "Prozesskategorie                           year parameter                         \n",
       "Altpapierrecycling (Pulper)                SQ   activation_duration    0.124792   \n",
       "Calciumcarbid-Herstellung (Lichtbogenofen) SQ   activation_duration    0.104167   \n",
       "Chlor-Alkali-Elektrolyse                   SQ   activation_duration    0.055205   \n",
       "Elektrostahlherstellung (Lichtbogenofen)   SQ   activation_duration    0.000243   \n",
       "Gießereien (Induktionsofen)                SQ   activation_duration    0.104167   \n",
       "...                                                                         ...   \n",
       "Warmwasserbereitstellung                   2030 shifting_duration     17.000000   \n",
       "                                           2050 shifting_duration     17.000000   \n",
       "                                           SQ   shifting_duration     17.500000   \n",
       "Klimakälte                                 2020 specific_investments  73.816154   \n",
       "                                           SQ   specific_investments  79.295725   \n",
       "\n",
       "                                                                            90%  \\\n",
       "Prozesskategorie                           year parameter                         \n",
       "Altpapierrecycling (Pulper)                SQ   activation_duration    0.045833   \n",
       "Calciumcarbid-Herstellung (Lichtbogenofen) SQ   activation_duration    0.083333   \n",
       "Chlor-Alkali-Elektrolyse                   SQ   activation_duration    0.000083   \n",
       "Elektrostahlherstellung (Lichtbogenofen)   SQ   activation_duration    0.000139   \n",
       "Gießereien (Induktionsofen)                SQ   activation_duration    0.083333   \n",
       "...                                                                         ...   \n",
       "Warmwasserbereitstellung                   2030 shifting_duration     17.600000   \n",
       "                                           2050 shifting_duration     17.600000   \n",
       "                                           SQ   shifting_duration     19.000000   \n",
       "Klimakälte                                 2020 specific_investments  69.151935   \n",
       "                                           SQ   specific_investments  74.123643   \n",
       "\n",
       "                                                                            95%  \\\n",
       "Prozesskategorie                           year parameter                         \n",
       "Altpapierrecycling (Pulper)                SQ   activation_duration    0.027083   \n",
       "Calciumcarbid-Herstellung (Lichtbogenofen) SQ   activation_duration    0.083333   \n",
       "Chlor-Alkali-Elektrolyse                   SQ   activation_duration    0.000000   \n",
       "Elektrostahlherstellung (Lichtbogenofen)   SQ   activation_duration    0.000076   \n",
       "Gießereien (Induktionsofen)                SQ   activation_duration    0.083333   \n",
       "...                                                                         ...   \n",
       "Warmwasserbereitstellung                   2030 shifting_duration     17.800000   \n",
       "                                           2050 shifting_duration     17.800000   \n",
       "                                           SQ   shifting_duration     19.500000   \n",
       "Klimakälte                                 2020 specific_investments  67.597195   \n",
       "                                           SQ   specific_investments  72.399616   \n",
       "\n",
       "                                                                      count  \\\n",
       "Prozesskategorie                           year parameter                     \n",
       "Altpapierrecycling (Pulper)                SQ   activation_duration     6.0   \n",
       "Calciumcarbid-Herstellung (Lichtbogenofen) SQ   activation_duration     6.0   \n",
       "Chlor-Alkali-Elektrolyse                   SQ   activation_duration    16.0   \n",
       "Elektrostahlherstellung (Lichtbogenofen)   SQ   activation_duration    12.0   \n",
       "Gießereien (Induktionsofen)                SQ   activation_duration     6.0   \n",
       "...                                                                     ...   \n",
       "Warmwasserbereitstellung                   2030 shifting_duration      16.0   \n",
       "                                           2050 shifting_duration      16.0   \n",
       "                                           SQ   shifting_duration      23.0   \n",
       "Klimakälte                                 2020 specific_investments    7.0   \n",
       "                                           SQ   specific_investments    9.0   \n",
       "\n",
       "                                                                            max  \\\n",
       "Prozesskategorie                           year parameter                         \n",
       "Altpapierrecycling (Pulper)                SQ   activation_duration    0.008333   \n",
       "Calciumcarbid-Herstellung (Lichtbogenofen) SQ   activation_duration    0.083333   \n",
       "Chlor-Alkali-Elektrolyse                   SQ   activation_duration    0.000000   \n",
       "Elektrostahlherstellung (Lichtbogenofen)   SQ   activation_duration    0.000000   \n",
       "Gießereien (Induktionsofen)                SQ   activation_duration    0.083333   \n",
       "...                                                                         ...   \n",
       "Warmwasserbereitstellung                   2030 shifting_duration     18.000000   \n",
       "                                           2050 shifting_duration     18.000000   \n",
       "                                           SQ   shifting_duration     20.000000   \n",
       "Klimakälte                                 2020 specific_investments  66.042455   \n",
       "                                           SQ   specific_investments  70.675588   \n",
       "\n",
       "                                                                            mean  \\\n",
       "Prozesskategorie                           year parameter                          \n",
       "Altpapierrecycling (Pulper)                SQ   activation_duration     0.768750   \n",
       "Calciumcarbid-Herstellung (Lichtbogenofen) SQ   activation_duration     0.166667   \n",
       "Chlor-Alkali-Elektrolyse                   SQ   activation_duration     0.251187   \n",
       "Elektrostahlherstellung (Lichtbogenofen)   SQ   activation_duration     0.584455   \n",
       "Gießereien (Induktionsofen)                SQ   activation_duration     0.166667   \n",
       "...                                                                          ...   \n",
       "Warmwasserbereitstellung                   2030 shifting_duration      11.326923   \n",
       "                                           2050 shifting_duration      10.442308   \n",
       "                                           SQ   shifting_duration      10.738971   \n",
       "Klimakälte                                 2020 specific_investments   95.792472   \n",
       "                                           SQ   specific_investments  279.650472   \n",
       "\n",
       "                                                                             min  \\\n",
       "Prozesskategorie                           year parameter                          \n",
       "Altpapierrecycling (Pulper)                SQ   activation_duration     3.000000   \n",
       "Calciumcarbid-Herstellung (Lichtbogenofen) SQ   activation_duration     0.250000   \n",
       "Chlor-Alkali-Elektrolyse                   SQ   activation_duration     1.000000   \n",
       "Elektrostahlherstellung (Lichtbogenofen)   SQ   activation_duration     3.000000   \n",
       "Gießereien (Induktionsofen)                SQ   activation_duration     0.250000   \n",
       "...                                                                          ...   \n",
       "Warmwasserbereitstellung                   2030 shifting_duration       4.000000   \n",
       "                                           2050 shifting_duration       2.500000   \n",
       "                                           SQ   shifting_duration       0.625000   \n",
       "Klimakälte                                 2020 specific_investments  150.354077   \n",
       "                                           SQ   specific_investments  729.750000   \n",
       "\n",
       "                                                                             std  \n",
       "Prozesskategorie                           year parameter                         \n",
       "Altpapierrecycling (Pulper)                SQ   activation_duration     1.128602  \n",
       "Calciumcarbid-Herstellung (Lichtbogenofen) SQ   activation_duration     0.074536  \n",
       "Chlor-Alkali-Elektrolyse                   SQ   activation_duration     0.310194  \n",
       "Elektrostahlherstellung (Lichtbogenofen)   SQ   activation_duration     0.924036  \n",
       "Gießereien (Induktionsofen)                SQ   activation_duration     0.074536  \n",
       "...                                                                          ...  \n",
       "Warmwasserbereitstellung                   2030 shifting_duration       5.863322  \n",
       "                                           2050 shifting_duration       6.200191  \n",
       "                                           SQ   shifting_duration       7.580785  \n",
       "Klimakälte                                 2020 specific_investments   39.602507  \n",
       "                                           SQ   specific_investments  295.387668  \n",
       "\n",
       "[3024 rows x 13 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_params_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original routine (KEEP FOR NOW; REMOVE WHEN UPDATE IS WORKING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# parameters_dict holds the data for all demand response parameters, sectors and years.\n",
    "# It is indexed by the parameters.\n",
    "# parameters_dict_nojoin is basically the same, but without filtering out \n",
    "# resp. joining duplicate values for different sectors.\n",
    "parameters_dict = {}\n",
    "parameters_dict_nojoin = {}\n",
    "\n",
    "# Count the number of params for which no data is available\n",
    "count_ignored = 0\n",
    "\n",
    "for parameter in parameters_groups_dict.keys():\n",
    "    parameters_dict[parameter] = pd.DataFrame(columns=[\"sector\", \"count\", \"mean\", \"std\", \n",
    "                                                       \"min\", \"25%\", \"50%\", \"75%\", \"max\", \"year_key\"])\n",
    "    for year in years:\n",
    "        for sector in sectors:\n",
    "            try:\n",
    "                new_df = pd.read_excel(path_folder_stats+parameter+\"_\"+sector+\"_stats.xlsx\", \n",
    "                                       sheet_name=year, \n",
    "                                       index_col=0).T\n",
    "                new_df[\"sector\"] = sector\n",
    "                new_df[\"year_key\"] = year\n",
    "                parameters_dict[parameter] = pd.concat([parameters_dict[parameter], new_df], sort=\"False\")\n",
    "            except:\n",
    "                count_ignored += 1\n",
    "                continue\n",
    "        \n",
    "        # Filter out the categories to be used and temporarily change index for further filtering\n",
    "        parameters_dict[parameter] = parameters_dict[parameter][parameters_dict[parameter].index.isin(categories.index)]\n",
    "        parameters_dict[parameter] = parameters_dict[parameter].set_index([parameters_dict[parameter].index, \"sector\"], \n",
    "                                                                          drop=False)\n",
    "        # Drop certain load management categories for certain sectors only\n",
    "        for k, v in to_drop.items():\n",
    "            if ((k, v)) in parameters_dict[parameter].index:\n",
    "                parameters_dict[parameter] = parameters_dict[parameter].drop(((k, v)))\n",
    "        parameters_dict[parameter] = parameters_dict[parameter].set_index(\n",
    "            parameters_dict[parameter].index.get_level_values(0))\n",
    "        parameters_dict[parameter].index.name = \"Prozess\"\n",
    "        \n",
    "        parameters_dict_nojoin[parameter] = parameters_dict[parameter]\n",
    "\n",
    "    # Combine demand response categories for tcs and and sector\n",
    "    if join_duplicates:\n",
    "        # Remove duplicate values in index for tcs and households sector\n",
    "        parameters_dict[parameter] = parameters_dict[parameter].set_index([parameters_dict[parameter].index, \"year_key\"])\n",
    "        duplicates = (parameters_dict[parameter].index.duplicated(keep=False) \n",
    "                      & parameters_dict[parameter][\"sector\"].isin([\"hoho\", \"tcs\"]))\n",
    "\n",
    "        # Get duplicates within duplicates (i.e. filter out the values where \n",
    "        # the same demand response categories occur in tcs and industry sector)\n",
    "        duplicates_df = parameters_dict[parameter][duplicates]\n",
    "        duplicates_df = duplicates_df[duplicates_df.index.duplicated(keep=False)]\n",
    "        \n",
    "        # Filter out the remaining duplicate values from original DataFrame\n",
    "        keys = list(duplicates_df.columns.values)\n",
    "        i1 = parameters_dict[parameter].set_index(keys).index\n",
    "        i2 = duplicates_df.set_index(keys).index\n",
    "        no_duplicates_df = parameters_dict[parameter][~i1.isin(i2)]\n",
    "        \n",
    "        # Assign certain demand response categories to a mix of tcs+hoho sector\n",
    "        for el in to_join:\n",
    "            if el in no_duplicates_df.index:\n",
    "                no_duplicates_df[\"help_sector\"] = no_duplicates_df[\"sector\"].apply(lambda x: \n",
    "                                                                                   x if x == \"ind\" else \"tcs+hoho\")\n",
    "                no_duplicates_df.loc[el, \"sector\"] = no_duplicates_df.loc[el, \"help_sector\"].values\n",
    "                no_duplicates_df = no_duplicates_df.drop([\"help_sector\"], axis=1)\n",
    "                \n",
    "        # drop index for grouping\n",
    "        duplicates_df = duplicates_df.reset_index(drop=False)\n",
    "        no_duplicates_df = no_duplicates_df.reset_index(drop=False)\n",
    "\n",
    "        if not duplicates_df.empty:\n",
    "\n",
    "            # Aggregate values for the remaining duplicates dependent on the kind of parameter\n",
    "            if parameters_groups_dict[parameter][0] == \"mean\":\n",
    "                duplicates_df = group_potential(duplicates_df, grouping_cols=[\"Prozess\", \"year_key\"], \n",
    "                                                mean_cols=[\"25%\", \"50%\", \"75%\", \"mean\", \"std\"], sum_cols=[\"count\"],\n",
    "                                                min_cols=[\"min\"], max_cols=[\"max\"], add_cluster=False, \n",
    "                                                weighted_ave=False)\n",
    "            else:\n",
    "                duplicates_df = group_potential(duplicates_df, grouping_cols=[\"Prozess\", \"year_key\"], \n",
    "                                                sum_cols=[\"min\", \"max\", \"25%\", \"50%\", \"75%\", \"mean\", \"std\", \"count\"],\n",
    "                                                add_cluster=False, weighted_ave=False)\n",
    "\n",
    "            # Add sector information again and concat original DataFrame and the aggregated remaining duplicates again\n",
    "            duplicates_df[\"sector\"] = \"tcs+hoho\"\n",
    "        \n",
    "        parameters_dict[parameter] = pd.concat([no_duplicates_df, duplicates_df], \n",
    "                                               sort=False).set_index(\"Prozess\")\n",
    "\n",
    "    # Change the order of preference for certain parameters\n",
    "    if parameters_groups_dict[parameter][1] == \"swap\":\n",
    "        try:\n",
    "            parameters_dict[parameter][[\"min_copy\",\"25%_copy\", \n",
    "                                        \"75%_copy\", \"max_copy\"]] = parameters_dict[parameter][[\"min\", \"25%\", \n",
    "                                                                                               \"75%\", \"max\"]]\n",
    "            parameters_dict[parameter][[\"min\", \"25%\", \n",
    "                                        \"75%\", \"max\"]] = parameters_dict[parameter][[\"max_copy\", \"75%_copy\", \n",
    "                                                                                     \"25%_copy\", \"min_copy\"]]\n",
    "            parameters_dict[parameter] = parameters_dict[parameter].drop([\"min_copy\",\"25%_copy\", \n",
    "                                                                          \"75%_copy\", \"max_copy\"], axis=1)\n",
    "\n",
    "            parameters_dict_nojoin[parameter][[\"min_copy\",\"25%_copy\", \n",
    "                                               \"75%_copy\", \"max_copy\"]] = parameters_dict_nojoin[parameter][[\"min\", \"25%\", \n",
    "                                                                                                             \"75%\", \"max\"]]\n",
    "            parameters_dict_nojoin[parameter][[\"min\", \"25%\", \n",
    "                                               \"75%\", \"max\"]] = parameters_dict_nojoin[parameter][[\"max_copy\", \"75%_copy\", \n",
    "                                                                                                   \"25%_copy\", \"min_copy\"]]\n",
    "            parameters_dict_nojoin[parameter] = parameters_dict_nojoin[parameter].drop([\"min_copy\",\"25%_copy\", \n",
    "                                                                                        \"75%_copy\", \"max_copy\"], axis=1)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "if join_duplicates:\n",
    "    sectors.append(\"tcs+hoho\")\n",
    "\n",
    "print(\"Overall number of params (sectors, years, params): \"+str(len(parameters_groups_dict) * (len(sectors) -1) * len(years)))\n",
    "print(\"Number of params not eligible for evaluation: \"+str(count_ignored))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Show an excerpt (example: overall positive shifting potential)\n",
    "parameters_dict[\"potential_pos_overall\"][parameters_dict[\"potential_pos_overall\"][\"year_key\"] == \"SQ\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and manipulate data for further usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put together parameter info and fill missing values\n",
    "* Put together all parameter information for a given sector in order to prepare for clustering\n",
    "* Deal with missing values:\n",
    "    * Calculate averages per sector and assing these.\n",
    "    * If information is still lacking for the status quo, fill remaining values with 0s for the status quo year. Since this is only applicable to the parameters activation duration, regeneration duration and variable costs, this causes no problems. Variable costs values of 0 are replaced later.\n",
    "* Round data to two decimal places.\n",
    "\n",
    "> _Note: In general, there are several options possible for dealing with missing data:_\n",
    "> * Drop missing values &rarr; Is not recommended and would eliminate all data\n",
    "> * Fill NaN values (and round data)\n",
    ">    * Use 0 values to fill NaN &rarr; changes the cluster decision itself \n",
    ">    * Use averages per sector &rarr; changes the cluster decision itself (approach is used at the moment)\n",
    ">    * Use imputations: https://scikit-learn.org/stable/modules/impute.html &rarr; state of the art but easy to make mistakes\n",
    ">    * Alternative to filling missing values: Partial cluster analysis (not supported by scikit-learn).\n",
    ">\n",
    "> See for options to deal with missing data: https://www.displayr.com/5-ways-deal-missing-data-cluster-analysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original routing (KEEP FOR NOW; REMOVE WHEN UPDATE IS WORKING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a separate DataFrame for every year key and store it in potential dict (indexed by year keys)\n",
    "# The clustering is carried out based on status quo data\n",
    "potential_dict = {}\n",
    "potential_dict_nojoin = {}\n",
    "\n",
    "for year in years:\n",
    "    # Create a DataFrame to store the information on all demand response parameters\n",
    "    df_potential = pd.DataFrame(columns=[\"Prozess\", \"sector\", \"year_key\"]).set_index(\"Prozess\")\n",
    "    df_potential_nojoin = pd.DataFrame(columns=[\"Prozess\", \"sector\", \"year_key\"]).set_index(\"Prozess\")\n",
    "\n",
    "    cols_new = cols.copy()\n",
    "    cols_new.extend([\"sector\", \"year_key\"])\n",
    "\n",
    "    # Put together the information for all demand response parameters\n",
    "    for k, v in parameters_dict.items():\n",
    "        try:\n",
    "            to_merge = v[cols_new][v[\"year_key\"] == year]\n",
    "            if not to_merge.empty:\n",
    "                df_potential = pd.merge(df_potential, to_merge, on=[\"Prozess\", \"sector\", \"year_key\"], \n",
    "                                        suffixes=[\"\",\"_\"+k], how=\"outer\")\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    # Put together the information for all demand response parameters (not joined version)\n",
    "    for k, v in parameters_dict_nojoin.items():\n",
    "        try:\n",
    "            to_merge = v[cols_new][v[\"year_key\"] == year]\n",
    "            if not to_merge.empty:\n",
    "                df_potential_nojoin = pd.merge(df_potential_nojoin, to_merge, on=[\"Prozess\", \"sector\", \"year_key\"], \n",
    "                                               suffixes=[\"\",\"_\"+k], how=\"outer\")\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Rename data for the first parameter (activation duration) which is lost in the merging process\n",
    "    # Set process and sector as index to obtain unique values\n",
    "    df_potential = df_potential.rename(columns={col: col+\"_\"+list(parameters_groups_dict.keys())[0] \n",
    "                                                for col in cols})\n",
    "    \n",
    "    df_potential_nojoin = df_potential_nojoin.rename(columns={col: col+\"_\"+list(parameters_groups_dict.keys())[0] \n",
    "                                                          for col in cols})\n",
    "    \n",
    "    potential_dict[year] = df_potential.set_index([df_potential.index, \"sector\"])\n",
    "    potential_dict_nojoin[year] = df_potential_nojoin.set_index([df_potential_nojoin.index, \"sector\"])\n",
    "    \n",
    "    # Fill Nan values and round:\n",
    "    # The mean values per sector and year are assigned to nans. \n",
    "    # If data is lacking completely, 0s are used to fill up the gaps (only for the status quo)\n",
    "    for sector in sectors:\n",
    "        median = potential_dict[year][potential_dict[year].index.get_level_values(1) == sector].median()\n",
    "        potential_dict[year][potential_dict[year].index.get_level_values(1) == sector] = \\\n",
    "            potential_dict[year][potential_dict[year].index.get_level_values(1) == sector].fillna(median)\n",
    "        if year == \"SQ\":\n",
    "            potential_dict[year] = potential_dict[year].fillna(0)\n",
    "        \n",
    "        median_nj = potential_dict_nojoin[year][potential_dict_nojoin[year].index.get_level_values(1) == sector].median()\n",
    "        potential_dict_nojoin[year][potential_dict_nojoin[year].index.get_level_values(1) == sector] = \\\n",
    "            potential_dict_nojoin[year][potential_dict_nojoin[year].index.get_level_values(1) == sector].fillna(median_nj)\n",
    "        if year == \"SQ\":\n",
    "            potential_dict_nojoin[year] = potential_dict_nojoin[year].fillna(0)\n",
    "\n",
    "    potential_dict[year] = potential_dict[year].round(2)\n",
    "    potential_dict_nojoin[year] = potential_dict_nojoin[year].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_dict[\"SQ\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do a correlation analysis of parameters in order to derive cluster parameters\n",
    "General idea:\n",
    "* Determine  pairwhise correlation in order to determine which parameters to use for clustering.\n",
    "* If correlation between to parameters is above 0.8, choose one of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display correlation matrix\n",
    "corr_matrix = potential_dict[\"SQ\"].corr().round(2)\n",
    "display(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the top 10 largest correlations\n",
    "print(\"Top Absolute Correlations\")\n",
    "print(75 * \"-\")\n",
    "display(get_top_abs_correlations(corr_matrix, n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_corr_series = get_top_abs_correlations(corr_matrix, threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_dict = {}\n",
    "ix1_slice_end = top_corr_series.index.get_level_values(0).str.slice(start=4)\n",
    "ix2_slice_end = top_corr_series.index.get_level_values(1).str.slice(start=4)\n",
    "ix1_slice_start = top_corr_series.index.get_level_values(0).str.slice(stop=4)\n",
    "ix2_slice_start = top_corr_series.index.get_level_values(1).str.slice(stop=4) \n",
    "\n",
    "corr_dict[\"same_params\"] = top_corr_series[ix1_slice_end == ix2_slice_end]\n",
    "corr_dict[\"different_params\"] = top_corr_series[ix1_slice_end != ix2_slice_end]\n",
    "corr_dict[\"only_medians\"] = top_corr_series[(ix1_slice_end != ix2_slice_end)\n",
    "                                            & (ix1_slice_start == \"50%_\") & (ix2_slice_start == \"50%_\")]\n",
    "\n",
    "if write_outputs:\n",
    "    write_multiple_sheets(corr_dict, path_folder_parameterization, filename_corr_out + \".xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show heatmap for correlations\n",
    "fig, ax = plt.subplots(figsize=(20, 15))\n",
    "_ = sns.heatmap(corr_matrix, cmap=\"RdGy\", vmin=-1.0, vmax=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create overall data set and interpolate missing values prior to clustering\n",
    "Overview on approach used:\n",
    "1. Create an **overall data set** and slice the parameters of interest.\n",
    "2. Introduce **linear interpolation** for potential and cost parameters:\n",
    "    * Potential parameters: The values for the status quo, 2030 and 2050 are used. Linear interpolation is made in between.\n",
    "    * Cost parameters: The minimum cost value is assigned to 2050 since cost increases seem pretty unlikely. Linear interpolation is made between the status quo and 2050 (minimum cost value). Costs of zero are set to 0.01 in order to attribute cost to DR and prevent numeric effects disturbing DR utilization in the model.\n",
    "3. **Assign remaining** parameters (mostly time-related ones) the same as for the status quo for every year.\n",
    "\n",
    "\n",
    "> _Note: There is quite some room for **further improvements**._\n",
    "> * _Development of positive and negative potentials should be in line with each other which doesn't have to be the case in the approach used._\n",
    "> * _General trends in parameter development could be identified (increase / decrease / constant) by some other systematic like linear regression (some code snippets are kept, but not used for now here)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create overall data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce a DataFrame to store overall data for all years\n",
    "potential_all_years = pd.DataFrame()\n",
    "potential_all_years_nojoin = pd.DataFrame()\n",
    "\n",
    "for year in years:\n",
    "    potential_dict[year][\"year_key\"] = year\n",
    "    potential_dict_nojoin[year][\"year_key\"] = year\n",
    "    potential_all_years = pd.concat([potential_all_years, potential_dict[year]])\n",
    "    potential_all_years_nojoin = pd.concat([potential_all_years_nojoin, potential_dict_nojoin[year]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all demand response categories\n",
    "dr_cats = potential_all_years.index.unique().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a **subset of the parameters** needs to be further analyzed:\n",
    "* Time-dependent parameters are assumed constant. These comprise:\n",
    "    * activation duration\n",
    "    * interference duration (both pos and neg) and shifting duration\n",
    "    * regeneration duration\n",
    "* Other parameters are not really resp. not directly used in the modeling approaches for DR. These comprise:\n",
    "    * average, minimum and maximum load\n",
    "* This leads to the following remaining parameters focussing on costs and potentials. Since the correlation analysis showed high redundandencies for the potential parameters, only the following remaining parameters will be further analyzed:\n",
    "    * potential positive overall\n",
    "    * potential negative overall\n",
    "    * installed capacity\n",
    "    * fixed and variable costs\n",
    "    * specific investments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_cats_nojoin = potential_all_years_nojoin.index.unique().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear interpolation for potential and costs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose parameters for which some adaptions are needed\n",
    "params_tbu_list = [\"potential_neg_overall\",\n",
    "                   \"potential_pos_overall\",\n",
    "                   \"installed_cap\",\n",
    "                   \"fixed_costs\", \"variable_costs\", \"specific_investments\"]\n",
    "\n",
    "params_tbu = create_parameter_combinations(params_tbu_list, cols)\n",
    "params_tbu.append(\"year_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the parameter values needed\n",
    "slice_pot = potential_all_years.loc[:,params_tbu]\n",
    "slice_pot_nojoin = potential_all_years_nojoin.loc[:,params_tbu]\n",
    "neg_pot_cols = create_parameter_combinations([\"potential_neg_overall\"], cols)\n",
    "slice_pot[neg_pot_cols] = slice_pot[neg_pot_cols].mul(-1)\n",
    "slice_pot_nojoin[neg_pot_cols] = slice_pot_nojoin[neg_pot_cols].mul(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform **interpolation on potential and costs parameters** in order to\n",
    "1. Fill data gaps and\n",
    "2. remove inplausibilities such as potentials changing very strong and not consistent within the five year intervalls used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define potential and cost cols\n",
    "pot_cols = create_parameter_combinations([\"potential_pos_overall\", \n",
    "                                          \"potential_neg_overall\", \n",
    "                                          \"installed_cap\"], cols)\n",
    "cost_cols = create_parameter_combinations([\"specific_investments\", \n",
    "                                           \"fixed_costs\", \n",
    "                                           \"variable_costs\"], cols)\n",
    "\n",
    "# Instanciate new DataFrame to store manipulated outputs\n",
    "slice_pot_new = pd.DataFrame(index=pd.MultiIndex.from_product([[],[],[]], \n",
    "                             names=[\"Prozess\", \"sector\", \"year_key\"]))\n",
    "slice_pot_new_nojoin = pd.DataFrame(index=pd.MultiIndex.from_product([[],[],[]], \n",
    "                                    names=[\"Prozess\", \"sector\", \"year_key\"]))\n",
    "\n",
    "# Create a list of DataFrames to concat\n",
    "to_concat = [slice_pot_new]\n",
    "to_concat_nojoin = [slice_pot_new_nojoin]\n",
    "\n",
    "for el in dr_cats:\n",
    "    df = slice_pot.loc[el].set_index(\"year_key\", append=True)\n",
    "\n",
    "    # Check potential values for interpolation\n",
    "    try:\n",
    "        pot_vals_SQ = df.loc[(el[0], el[1], \"SQ\"), pot_cols]\n",
    "    except KeyError:\n",
    "        continue\n",
    "    try:\n",
    "        pot_vals_2030 = df.loc[(el[0], el[1], \"2030\"), pot_cols]\n",
    "    except:\n",
    "        pot_vals_2030 = np.nan\n",
    "    try:\n",
    "        pot_vals_2050 = df.loc[(el[0], el[1], \"2050\"), pot_cols]\n",
    "    except:\n",
    "        pot_vals_2050 = pot_vals_SQ\n",
    "    \n",
    "    # Assign potential values for 2030 and 2050 and interpolate in between\n",
    "    multi_ix = pd.MultiIndex.from_product([[el[0]], [el[1]], years], \n",
    "                                          names=[\"Prozess\", \"sector\", \"year_key\"])\n",
    "    new_df = pd.DataFrame(index=multi_ix)\n",
    "    new_df.loc[(el[0], el[1], \"SQ\"), pot_cols] = pot_vals_SQ\n",
    "    new_df.loc[(el[0], el[1], \"2030\"), pot_cols] = pot_vals_2030\n",
    "    new_df.loc[(el[0], el[1], \"2050\"), pot_cols] = pot_vals_2050\n",
    "\n",
    "    # Check cost values for interpolation\n",
    "    cost_vals_SQ = df.loc[(el[0], el[1], \"SQ\"), cost_cols]     \n",
    "    min_costs = df[cost_cols].min()\n",
    "\n",
    "    # Assign minimum value for 2050 and interpolate in between\n",
    "    new_df.loc[(el[0], el[1], \"SQ\"), cost_cols] = cost_vals_SQ.fillna(0.01)\n",
    "    new_df.loc[(el[0], el[1], \"2050\"), cost_cols] = min_costs\n",
    "    new_df[cost_cols] = new_df[cost_cols].replace({0: 0.01})\n",
    "    new_df = new_df.interpolate()\n",
    "\n",
    "    new_df = new_df.interpolate()\n",
    "    to_concat.append(new_df)\n",
    "\n",
    "slice_pot_new = pd.concat([el for el in to_concat], levels=([0, 1, 2]))    \n",
    "\n",
    "# nojoin\n",
    "for el in dr_cats_nojoin:\n",
    "    df = slice_pot_nojoin.loc[el].set_index(\"year_key\", append=True)\n",
    "\n",
    "    # Check potential values for interpolation\n",
    "    try:\n",
    "        pot_vals_SQ = df.loc[(el[0], el[1], \"SQ\"), pot_cols]\n",
    "    except KeyError:\n",
    "        continue\n",
    "    try:\n",
    "        pot_vals_2030 = df.loc[(el[0], el[1], \"2030\"), pot_cols]\n",
    "    except:\n",
    "        pot_vals_2030 = np.nan\n",
    "    try:\n",
    "        pot_vals_2050 = df.loc[(el[0], el[1], \"2050\"), pot_cols]\n",
    "    except:\n",
    "        pot_vals_2050 = pot_vals_SQ\n",
    "    \n",
    "    # Assign potential values for 2030 and 2050 and interpolate in between\n",
    "    multi_ix = pd.MultiIndex.from_product([[el[0]], [el[1]], years], \n",
    "                                          names=[\"Prozess\", \"sector\", \"year_key\"])\n",
    "    new_df = pd.DataFrame(index=multi_ix)\n",
    "    new_df.loc[(el[0], el[1], \"SQ\"), pot_cols] = pot_vals_SQ\n",
    "    new_df.loc[(el[0], el[1], \"2030\"), pot_cols] = pot_vals_2030\n",
    "    new_df.loc[(el[0], el[1], \"2050\"), pot_cols] = pot_vals_2050\n",
    "\n",
    "    # Check cost values for interpolation\n",
    "    cost_vals_SQ = df.loc[(el[0], el[1], \"SQ\"), cost_cols]     \n",
    "    min_costs = df[cost_cols].min()\n",
    "\n",
    "    # Assign minimum value for 2050 and interpolate in between\n",
    "    new_df.loc[(el[0], el[1], \"SQ\"), cost_cols] = cost_vals_SQ.fillna(0.01)\n",
    "    new_df.loc[(el[0], el[1], \"2050\"), cost_cols] = min_costs\n",
    "    new_df[cost_cols] = new_df[cost_cols].replace({0: 0.01})\n",
    "    new_df = new_df.interpolate().round(2)\n",
    "    \n",
    "    new_df = new_df.interpolate()\n",
    "    to_concat_nojoin.append(new_df)\n",
    "    \n",
    "slice_pot_new_nojoin = pd.concat([el for el in to_concat_nojoin], levels=([0, 1, 2]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_pot_new[cost_cols].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternative trend analysis for DR parameters**:\n",
    "> _NOTE: This section is work in progress and contains some open questions._<br>\n",
    "> _It will be skipped for that reason (unless it is explicitly stated not to be skipped)._\n",
    "* Potential parameters:\n",
    "    * If $ r^2 \\geq 0.8 $ &rarr; apply linear regression.\n",
    "    * Use other approach yet to be defined...\n",
    "* Cost parameters: Suitable approach yet to be defined..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: use linear regression to analyze trends for dr categories\n",
    "if not skip_dev:\n",
    "    # \"year_key\" has to be an integer for that purpose\n",
    "    slice_pot[\"year_key\"] = slice_pot[\"year_key\"].replace({\"SQ\": \"2015\"})\n",
    "    slice_pot[\"year_key\"] = slice_pot[\"year_key\"].astype(int)\n",
    "    \n",
    "    for el in dr_cats:\n",
    "        for pot_col in [\"50%_potential_neg_overall\", \"50%_potential_pos_overall\"]:\n",
    "            slope, intercept, r_value, p_value, std_err = linregress(\n",
    "                slice_pot.loc[el, \"year_key\"].values,\n",
    "                slice_pot.loc[el, pot_col].values)\n",
    "            if r_value ** 2 >= 0.8:\n",
    "                slice_pot.loc[el, pot_col][slice_pot.loc[el, \"year_key\"] != 2015] \\\n",
    "                    = intercept + slope * slice_pot.loc[el, \"year_key\"]\n",
    "\n",
    "        #plt.scatter(slice_pot.loc[el, \"year_key\"].values, \n",
    "        #            slice_pot.loc[el, pot_col].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment of remaining (time-related parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign remaining (mostly time-related) parameters\n",
    "params_remaining_list = [\"interference_duration_neg\",\n",
    "                         \"interference_duration_pos\", \n",
    "                         \"maximum_activations_year\", \n",
    "                         \"regeneration_duration\", \n",
    "                         \"shifting_duration\"]\n",
    "\n",
    "params_remaining = create_parameter_combinations(params_remaining_list, cols)\n",
    "params_remaining.append(\"year_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the parameter values needed\n",
    "slice_pot = potential_all_years.loc[:,params_remaining]\n",
    "slice_pot_nojoin = potential_all_years_nojoin.loc[:,params_remaining]\n",
    "slice_pot.set_index(\"year_key\", append=True, drop=False, inplace=True)\n",
    "slice_pot_nojoin.set_index(\"year_key\", append=True, drop=False, inplace=True)\n",
    "params_remaining.remove(\"year_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_pot_new.loc[slice_pot_new.index.get_level_values(2) == \"SQ\", params_remaining] =\\\n",
    "    slice_pot.loc[slice_pot.index.get_level_values(2) == \"SQ\", params_remaining]\n",
    "slice_pot_new.fillna(method=\"ffill\", inplace=True)\n",
    "\n",
    "slice_pot_new_nojoin.loc[slice_pot_new_nojoin.index.get_level_values(2) == \"SQ\", params_remaining] =\\\n",
    "    slice_pot_nojoin.loc[slice_pot_nojoin.index.get_level_values(2) == \"SQ\", params_remaining]\n",
    "slice_pot_new_nojoin.fillna(method=\"ffill\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearrange to dict-based data structure\n",
    "> _NOTE: In general, this is not a necessity, but utilized here in order not to have to revert the entire ensuing clustering routine (which was initially build up upon the dict-based data structure)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_pot_new = slice_pot_new.reset_index(level=2)\n",
    "slice_pot_new_nojoin = slice_pot_new_nojoin.reset_index(level=2)\n",
    "\n",
    "for year in years:\n",
    "    potential_dict[year] = slice_pot_new[slice_pot_new[\"year_key\"] == year].drop(\"year_key\", axis=1)\n",
    "    potential_dict_nojoin[year] = slice_pot_new_nojoin[slice_pot_new_nojoin[\"year_key\"] == year].drop(\"year_key\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_dict[\"SQ\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_dict_nojoin[\"2050\"].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_dict[\"SQ\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster data\n",
    "* Do a clustering using K-Means and the cluster parameters defined in the above parameter settings (alternative: agglomerative clustering using ward linkage).\n",
    "* Default: Cluster within sectors by shifting times, positive interference duration, variable costs as well as specific investments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the actual clustering\n",
    "* Create a list of the cluster parameters\n",
    "* Determine the number of clusters per sector\n",
    "* Perform a K-Means clustering on the data (alternative: agglomerative clustering using ward linkage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "cluster_parameters_list = create_parameter_combinations(cluster_parameters, [\"50%\"])\n",
    "cluster_parameters_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the column to store the cluster label\n",
    "potential_dict[\"SQ\"][\"cluster\"] = 0\n",
    "\n",
    "print(f\"Number of clusters for {cluster_algo} clustering\")\n",
    "\n",
    "# Increase cluster numbers by (arbitratily chosen) increments\n",
    "# in order not to overwrite cluster information\n",
    "iter = 0\n",
    "increments = [1000, 100, 10, 1]\n",
    "\n",
    "# Iterate over the sectors and create clusters within the sectors\n",
    "for sector in potential_dict[\"SQ\"].index.get_level_values(1).unique():\n",
    "    # Determine the number of clusters per sector and print it\n",
    "    n = math.ceil(potential_dict[\"SQ\"][potential_dict[\"SQ\"].index.get_level_values(1) == sector].shape[0]*share_clusters)\n",
    "    print(sector+\":\\t\\t\"+str(n))\n",
    "    \n",
    "    if cluster_algo == \"KMeans\":\n",
    "        # Do the actual clustering and assign the cluster labels\n",
    "        #display(potential_dict[\"SQ\"].loc[potential_dict[\"SQ\"].index.get_level_values(1) == sector])\n",
    "        potential_dict[\"SQ\"].loc[potential_dict[\"SQ\"].index.get_level_values(1) == sector, \"cluster\"] = \\\n",
    "            KMeans(n_clusters=n).fit(\n",
    "                potential_dict[\"SQ\"].loc[potential_dict[\"SQ\"].index.get_level_values(1) == sector, \n",
    "                                         cluster_parameters_list].values).labels_ + 1\n",
    "        \n",
    "    elif cluster_algo == \"ward\":\n",
    "        # Do the actual clustering and assign the cluster labels\n",
    "        potential_dict[\"SQ\"].loc[potential_dict[\"SQ\"].index.get_level_values(1) == sector, \"cluster\"] = \\\n",
    "            AgglomerativeClustering(n_clusters=n, linkage=\"ward\").fit(\n",
    "                potential_dict[\"SQ\"].loc[potential_dict[\"SQ\"].index.get_level_values(1) == sector, cluster_parameters_list].values).labels_ + 1       \n",
    "\n",
    "    # Increment the cluster labels to avoid overwriting in the next iteration (K-Means always starts with zero)\n",
    "    potential_dict[\"SQ\"].loc[potential_dict[\"SQ\"][\"cluster\"] != 0, \"cluster\"] += n * increments[iter]\n",
    "    iter += 1\n",
    "    #display(potential_dict[\"SQ\"].loc[potential_dict[\"SQ\"].index.get_level_values(1) == sector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Print the number of unique cluster labels (cross check)\n",
    "potential_dict[\"SQ\"][\"cluster\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster assignment**:\n",
    "* Assign cluster information for future years (_Note: simple assignment does an index mapping_).\n",
    "* Furthermore, assing cluster information to data set in which tcs and hoho applications haven't been joined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    if not year == \"SQ\":\n",
    "        potential_dict[year][\"cluster\"] = potential_dict[\"SQ\"][\"cluster\"]\n",
    "        \n",
    "    # Add cluster information for data not joined\n",
    "    potential_dict_nojoin[year][\"cluster\"] = potential_dict[year][\"cluster\"]\n",
    "\n",
    "    # Add cluster information for categories for which it is missing, i.e. categories in tcs+hoho sector\n",
    "    # Determine indices to use for selecting cluster numbers\n",
    "    no_cluster_ixs = list(set(potential_dict_nojoin[year][\n",
    "        potential_dict_nojoin[year][\"cluster\"].isna()].index.get_level_values(0)))\n",
    "    no_cluster_ixs = [(i, j) for i in no_cluster_ixs for j in [\"tcs+hoho\"]]\n",
    "\n",
    "    # Use only the first index level to be able to assign values (second index levels won't match)\n",
    "    clusters_to_use = potential_dict[year].loc[no_cluster_ixs, \"cluster\"].to_frame()\n",
    "    clusters_to_use = clusters_to_use.set_index(clusters_to_use.index.get_level_values(0))\n",
    "\n",
    "    # Use an excerpt of the overall DataFrame to assign the cluster info\n",
    "    excerpt = potential_dict_nojoin[year].loc[potential_dict_nojoin[year][\"cluster\"].isna(),\"cluster\"].to_frame()\n",
    "    excerpt = excerpt.reset_index().set_index(\"Prozess\")\n",
    "    excerpt[\"cluster\"] = clusters_to_use[\"cluster\"]\n",
    "    excerpt = excerpt.set_index(\"sector\", append=True)\n",
    "    potential_dict_nojoin[year].loc[potential_dict_nojoin[year][\"cluster\"].isna(),\"help_sector\"] = \"tcs+hoho\"\n",
    "    potential_dict_nojoin[year].loc[potential_dict_nojoin[year][\"cluster\"].isna(),\"cluster\"] = excerpt.loc[:,\"cluster\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_dict[\"2050\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_dict[\"SQ\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show and explore the clusters\n",
    "Print the clusters in order to visually inspect them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if print_clusters:\n",
    "    for el in np.sort(potential_dict[\"SQ\"][\"cluster\"].unique()):\n",
    "        print(20 * \"-\")\n",
    "        print(\"cluster number: \"+str(el))\n",
    "        print(20 * \"-\")\n",
    "        display(potential_dict[\"SQ\"][potential_dict[\"SQ\"][\"cluster\"] == el])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create transposed potential data sets for combining it with availability data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dicts to store the transposed (and not joined) potential data\n",
    "# as well as the availability time series\n",
    "potential_T_dict = {}\n",
    "potential_ind_T_dict = {}\n",
    "potential_tcs_T_dict = {}\n",
    "potential_hoho_T_dict = {}\n",
    "potential_tcs_hoho_T_dict = {}\n",
    "\n",
    "for year in years:\n",
    "    # Create transposed subsets of potential_dict_nojoin\n",
    "    potential_T_dict[year] = potential_dict_nojoin[year].T\n",
    "    potential_ind_T_dict[year] = potential_dict_nojoin[year][\n",
    "        potential_dict_nojoin[year].index.get_level_values(1) == \"ind\"].T\n",
    "    potential_tcs_T_dict[year] = potential_dict_nojoin[year][\n",
    "        potential_dict_nojoin[year].index.get_level_values(1) == \"tcs\"].T\n",
    "    potential_hoho_T_dict[year] = potential_dict_nojoin[year][\n",
    "        potential_dict_nojoin[year].index.get_level_values(1) == \"hoho\"].T\n",
    "    potential_tcs_hoho_T_dict[year] = potential_dict_nojoin[year][\n",
    "        potential_dict_nojoin[year].index.get_level_values(1) == \"tcs+hoho\"].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_T_dict[\"SQ\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_T_dict[\"2050\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "## Read in and match availability data\n",
    "\n",
    "* Read in the availability time series data.\n",
    "* Match the data to the respective demand response categories.\n",
    "        \n",
    "> __*Conceptual question:*__ \n",
    "> _Rethink, whether availability timeseries indexed by year do make sense and how this should be handled within (investment) model runs._\n",
    "> * _Pros: Different years (e.g. temperature or weekday patterns) could be depicted._\n",
    "> * _Cons: Some effort has to be made to achieve that_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read in availability time series data\n",
    "availability_ind_pos = pd.read_excel(path_folder_availability+filename_availability_in,\n",
    "                                     sheet_name = \"ind_pos\", header=0, index_col=0)\n",
    "\n",
    "availability_ind_neg = pd.read_excel(path_folder_availability+filename_availability_in,\n",
    "                                     sheet_name = \"ind_neg\", header=0, index_col=0)\n",
    "\n",
    "availability_tcs_pos = pd.read_excel(path_folder_availability+filename_availability_in,\n",
    "                                     sheet_name = \"tcs_pos\", header=0, index_col=0)\n",
    "\n",
    "availability_tcs_neg = pd.read_excel(path_folder_availability+filename_availability_in,\n",
    "                                     sheet_name = \"tcs_neg\", header=0, index_col=0)\n",
    "\n",
    "availability_hoho_pos = pd.read_excel(path_folder_availability+filename_availability_in,\n",
    "                                     sheet_name = \"hoho_pos\", header=0, index_col=0)\n",
    "\n",
    "availability_hoho_neg = pd.read_excel(path_folder_availability+filename_availability_in,\n",
    "                                     sheet_name = \"hoho_neg\", header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Map the demand response categories to the column names used\n",
    "# for the availability time series (naming conventions from the bachelor theses are widely used).\n",
    "availability_cats_ind_pos = {'normiertes\\nLRP Alu': ('Primäraluminiumelektrolyse', 'ind'),\n",
    "                            'normiertes\\nLRP CHL': ('Chlor-Alkali-Elektrolyse', 'ind'),\n",
    "                            'normiertes\\nLRP HS': ('Holz- und Zellstoffherstellung', 'ind'),\n",
    "                            'normiertes\\nLRP PM': ('Papiermaschinen', 'ind'),\n",
    "                            'normiertes\\nLRP ES': ('Elektrostahlherstellung (Lichtbogenofen)', 'ind'),\n",
    "                            'normiertes\\nLRP RZM': ('Zementherstellung', 'ind'),\n",
    "                            'normiertes\\nLRP KUZI': ('Kupfer- und Zinkherstellung (Elektrolyse)', 'ind'),\n",
    "                            'normiertes\\nLRP AP': ('Altpapierrecycling (Pulper)', 'ind')}\n",
    "\n",
    "availability_cats_ind_neg = {'normiertes\\nLZP Alu': ('Primäraluminiumelektrolyse', 'ind'),\n",
    "                            'normiertes\\nLZP CHL': ('Chlor-Alkali-Elektrolyse', 'ind'),\n",
    "                            'normiertes\\nLZP HS': ('Holz- und Zellstoffherstellung', 'ind'),\n",
    "                            'normiertes\\nLZP PM': ('Papiermaschinen', 'ind'),\n",
    "                            'normiertes\\nLZP ES': ('Elektrostahlherstellung (Lichtbogenofen)', 'ind'),\n",
    "                            'normiertes\\nLZP RZM': ('Zementherstellung', 'ind'),\n",
    "                            'normiertes\\nLZP KUZI': ('Kupfer- und Zinkherstellung (Elektrolyse)', 'ind'),\n",
    "                            'normiertes\\nLZP AP': ('Altpapierrecycling (Pulper)', 'ind')}\n",
    "\n",
    "# Note: Instead of KGR in the tcs sector, profiles for food retailing are introduced and used.\n",
    "availability_cats_tcs_pos = {'Lastabschaltung LÜ normiert': ('Belüftung', 'tcs'), \n",
    "                            'Lastabschaltung WVP normiert': ('Pumpenanwendungen in der Wasserversorgung', 'tcs'),\n",
    "                            'Lastabschaltung KGR normiert': ('Prozesskälte Handel', 'tcs'),\n",
    "                            'Lastabschaltung KÜ normiert': ('Kühlhäuser', 'tcs'),\n",
    "                            'Lastabschaltung KA normiert': ('Klimakälte', 'tcs'),  \n",
    "                            'Lastabschaltung EH normiert': ('Nachtspeicherheizungen', 'tcs'),\n",
    "                            'Lastabschaltung WP normiert': ('Wärmepumpen', 'tcs'), \n",
    "                            'Lastabschaltung WW normiert': ('Warmwasserbereitstellung', 'tcs')}\n",
    "\n",
    "availability_cats_tcs_neg = {'Lastzuschaltung LÜ normiert': ('Belüftung', 'tcs'),\n",
    "                             'Lastzuschaltung WVP normiert': ('Pumpenanwendungen in der Wasserversorgung', 'tcs'),\n",
    "                             'Lastzuschaltung KGR normiert': ('Prozesskälte Handel', 'tcs'),\n",
    "                             'Lastzuschaltung KÜ normiert': ('Kühlhäuser', 'tcs'),\n",
    "                             'Lastzuschaltung KA normiert': ('Klimakälte', 'tcs'), \n",
    "                             'Lastzuschaltung EH normiert': ('Nachtspeicherheizungen', 'tcs'),\n",
    "                             'Lastzuschaltung WP normiert': ('Wärmepumpen', 'tcs'), \n",
    "                             'Lastzuschaltung WW normiert': ('Warmwasserbereitstellung', 'tcs')}\n",
    "\n",
    "availability_cats_hoho_pos = {'Lastabschaltung KGR': ('Kühlschränke', 'hoho'), \n",
    "                              'Lastabschaltung WM': ('Waschmaschinen', 'hoho'), \n",
    "                              'Lastabschaltung WT': ('Wäschetrockner', 'hoho'),\n",
    "                              'Lastabschaltung GS': ('Geschirrspüler', 'hoho'), \n",
    "                              'Lastabschaltung NH normiert': ('Nachtspeicherheizungen', 'hoho'),\n",
    "                              'Lastabschaltung WP normiert': ('Wärmepumpen', 'hoho'), \n",
    "                              'Lastabschaltung UP': ('Heizungsumwälzpumpen', 'hoho'),\n",
    "                              'Lastabschaltung RK': ('Klimakälte', 'hoho'), \n",
    "                              'Lastabschaltung WW Tag': ('Warmwasserbereitstellung', 'hoho')}\n",
    "\n",
    "availability_cats_hoho_neg = {'Lastzuschaltung KGR normiert': ('Kühlschränke', 'hoho'),\n",
    "                              'Lastzuschaltung WM normiert': ('Waschmaschinen', 'hoho'),\n",
    "                              'Lastzuschaltung WT normiert': ('Wäschetrockner', 'hoho'),\n",
    "                              'Lastzuschaltung GS normiert': ('Geschirrspüler', 'hoho'), \n",
    "                              'Lastzuschaltung NH normiert': ('Nachtspeicherheizungen', 'hoho'),\n",
    "                              'Lastzuschaltung WP normiert': ('Wärmepumpen', 'hoho'),\n",
    "                              'Lastzuschaltung RK': ('Klimakälte', 'hoho'), \n",
    "                              'Lastzuschaltung WW normiert Tag': ('Warmwasserbereitstellung', 'hoho')}\n",
    "\n",
    "\n",
    "# Change column name of availability time series DataFrame\n",
    "availability_ind_pos = map_column_names(availability_ind_pos, availability_cats_ind_pos)\n",
    "availability_ind_neg = map_column_names(availability_ind_neg, availability_cats_ind_neg)\n",
    "availability_tcs_pos = map_column_names(availability_tcs_pos, availability_cats_tcs_pos)\n",
    "availability_tcs_neg = map_column_names(availability_tcs_neg, availability_cats_tcs_neg)\n",
    "availability_hoho_pos = map_column_names(availability_hoho_pos, availability_cats_hoho_pos)\n",
    "availability_hoho_neg = map_column_names(availability_hoho_neg, availability_cats_hoho_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __*Important TODO for availability: Improve proxies for availability!*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create availability time series for demand response categories for which availability info is missing\n",
    "\n",
    "In general, there are three options for filling up the missing availability information:\n",
    "1. Assign the existing availability time series of another process / demand response category due to large similarities (assumed).\n",
    "2. Assign a constant availability profile for the entire year.\n",
    "3. Create a synthetic load profile by defining availability factors for hours, weekdays and months and combining them.\n",
    "    \n",
    "The folowing availability information is used:\n",
    "* Industry sector:\n",
    "    * Foundries (German: \"Gießereien\") will be assigned the value for copper and zinc.\n",
    "    * Calcium carbide will be assigned the value for electric furnace steel.\n",
    "    * For the remaining categories, no profiles are available. As a first proxy, a constant availability profile is assumed.\n",
    "* Tcs sector:\n",
    "    * In the first place, for all categories not covered, a constant availability profile is assumed. _&rarr; Needs to be changed!_\n",
    "* Household sector:\n",
    "    * fride-freezer combinations as well as freezers will be assigned the same profile as fridges.\n",
    "    * For all remaining categories not covered, a constant availability profile is assumed. _&rarr; Needs to be changed!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use availability time series of existing categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Boadcast values from existing demand response categories\n",
    "\n",
    "# Industry sector\n",
    "availability_ind_pos[('Calciumcarbid-Herstellung (Lichtbogenofen)', 'ind')] = \\\n",
    "    availability_ind_pos[('Elektrostahlherstellung (Lichtbogenofen)', 'ind')]\n",
    "availability_ind_pos[('Gießereien (Induktionsofen)', 'ind')] = \\\n",
    "    availability_ind_pos[('Kupfer- und Zinkherstellung (Elektrolyse)', 'ind')]\n",
    "\n",
    "availability_ind_neg[('Calciumcarbid-Herstellung (Lichtbogenofen)', 'ind')] = \\\n",
    "    availability_ind_neg[('Elektrostahlherstellung (Lichtbogenofen)', 'ind')]\n",
    "availability_ind_neg[('Gießereien (Induktionsofen)', 'ind')] = \\\n",
    "    availability_ind_neg[('Kupfer- und Zinkherstellung (Elektrolyse)', 'ind')]\n",
    "\n",
    "# tcs\n",
    "availability_tcs_pos[('Prozesskälte', 'tcs')] =\\\n",
    "   availability_tcs_pos[('Kühlhäuser', 'tcs')] \n",
    "availability_tcs_neg[('Prozesskälte', 'tcs')] =\\\n",
    "   availability_tcs_neg[('Kühlhäuser', 'tcs')] \n",
    "\n",
    "# households\n",
    "availability_hoho_pos[('Kühl- und Gefrierkombinationen', 'hoho')] = \\\n",
    "    availability_hoho_pos[('Kühlschränke', 'hoho')]\n",
    "availability_hoho_pos[('Gefrierschränke und -truhen', 'hoho')] = \\\n",
    "    availability_hoho_pos[('Kühlschränke', 'hoho')]\n",
    "\n",
    "availability_hoho_neg[('Kühl- und Gefrierkombinationen', 'hoho')] = \\\n",
    "    availability_hoho_neg[('Kühlschränke', 'hoho')]\n",
    "availability_hoho_neg[('Gefrierschränke und -truhen', 'hoho')] = \\\n",
    "    availability_hoho_neg[('Kühlschränke', 'hoho')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine for which categories availability time series are still missing\n",
    "determine_missing_cols(potential_ind_T_dict[\"SQ\"], availability_ind_pos, sector=\"industry (positive)\")\n",
    "determine_missing_cols(potential_ind_T_dict[\"SQ\"], availability_ind_neg, sector=\"industry (negative)\")\n",
    "determine_missing_cols(potential_tcs_T_dict[\"SQ\"], availability_tcs_pos, sector=\"tcs (positive)\")\n",
    "determine_missing_cols(potential_tcs_T_dict[\"SQ\"], availability_tcs_neg, sector=\"tcs (negative)\")\n",
    "determine_missing_cols(potential_hoho_T_dict[\"SQ\"], availability_hoho_pos, sector=\"hoho (positive)\")\n",
    "determine_missing_cols(potential_hoho_T_dict[\"SQ\"], availability_hoho_neg, sector=\"hoho (negative)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define synthetic profiles for availability\n",
    "Define **synthetic profiles** for the remaining demand response categories missing availability information.\n",
    "These comprise of a factor for the hourly, daily (weekday information) and monthly patterns. The three are multiplied in order to obtain hourly availability information. A very similar approach is used in Gils (2015).\n",
    "\n",
    "> __*NOTE:*__\n",
    "> * _Synthetic profiles used in the following are mostly **dummy values** to be replaced!!!_\n",
    "> * _For a first \"quick'n'dirty\" model run, these are utilized, though._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define synthetic profiles for the missing categories\n",
    "# Create hours, weekday and month factors\n",
    "hours_dict_pos = {}\n",
    "days_dict_pos = {}\n",
    "months_dict_pos = {}\n",
    "\n",
    "hours_dict_neg = {}\n",
    "days_dict_neg = {}\n",
    "months_dict_neg = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign hourly profiles - positive\n",
    "hours = range(0, 24)\n",
    "\n",
    "# dummies\n",
    "hours_dummy_constant = [1] * 24\n",
    "hours_dummy_morning_evening_reduced = [0.8] * 6 + [1.0] * 12 + [0.8] * 6\n",
    "\n",
    "# ind\n",
    "hours_climate_cold_ind = [0.1, 0.1, 0.1, 0.2, 0.2, 0.2,\n",
    "                          0.3, 0.4, 0.5, 0.6, 0.8, 1.0,\n",
    "                          1.0, 1.0, 1.0, 1.0, 0.8, 0.7,\n",
    "                          0.6, 0.5, 0.3, 0.2, 0.1, 0.1]\n",
    "\n",
    "hours_dict_pos[('Klimakälte', 'ind')] = dict(zip(hours, hours_climate_cold_ind))\n",
    "hours_dict_pos[('Prozesskälte', 'ind')] = dict(zip(hours, hours_dummy_constant))\n",
    "hours_dict_pos[('Luftzerlegung', 'ind')] = dict(zip(hours, hours_dummy_constant))\n",
    "hours_dict_pos[('Kühlung (Lebensmittelindustrie)', 'ind')] = dict(zip(hours, hours_dummy_constant))\n",
    "hours_dict_pos[('Pumpenanwendungen', 'ind')] = dict(zip(hours, hours_dummy_morning_evening_reduced))\n",
    "hours_dict_pos[('Beleuchtung', 'ind')] = dict(zip(hours, hours_dummy_morning_evening_reduced))\n",
    "hours_dict_pos[('Druckluftanwendungen', 'ind')] = dict(zip(hours, hours_dummy_morning_evening_reduced))\n",
    "hours_dict_pos[('Belüftung', 'ind')] = dict(zip(hours, hours_dummy_morning_evening_reduced))\n",
    "\n",
    "# tcs\n",
    "hours_ava_light_tcs = [0.05, 0.05, 0.1, 0.1, 0.15, 0.2, \n",
    "                       0.6, 1.0, 1.0, 0.9, 0.9, 0.8, \n",
    "                       0.8, 0.7, 0.7, 0.7, 0.6, 0.7, \n",
    "                       0.5, 0.2, 0.15, 0.15, 0.11, 0.05]\n",
    "\n",
    "hours_dict_pos[('Prozesskälte', 'tcs')] = dict(zip(hours, hours_dummy_constant))\n",
    "hours_dict_pos[('Beleuchtung', 'tcs')] = dict(zip(hours, hours_dummy_morning_evening_reduced))\n",
    "hours_dict_pos[('Zerkleinerer', 'tcs')] = dict(zip(hours, hours_dummy_morning_evening_reduced))\n",
    "hours_dict_pos[('Prozesswärme', 'tcs')] = dict(zip(hours, hours_dummy_morning_evening_reduced))\n",
    "hours_dict_pos[('Abwasserbehandlung', 'tcs')] = dict(zip(hours, hours_dummy_morning_evening_reduced))\n",
    "\n",
    "# hoho\n",
    "hours_dict_pos[('Heizungsumwälzpumpen', 'hoho')] = dict(zip(hours, hours_dummy_morning_evening_reduced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign hourly profiles - negative\n",
    "hours = range(0, 24)\n",
    "\n",
    "# dummies\n",
    "hours_dummy_constant = [1] * 24\n",
    "hours_dummy_morning_evening_reduced = [1] * 6 + [0.8] * 12 + [1] * 6\n",
    "\n",
    "# ind\n",
    "hours_climate_cold_ind = [0.05, 0.05, 0.05, 0.1, 0.1, 0.1,\n",
    "                          0.15, 0.2, 0.2, 0.2, 0.2, 0,\n",
    "                          0, 0, 0, 0, 0.2, 0.2,\n",
    "                          0.2, 0.2, 0.15, 0.1, 0.05, 0.05]\n",
    "\n",
    "hours_dict_neg[('Klimakälte', 'ind')] = dict(zip(hours, hours_climate_cold_ind))\n",
    "hours_dict_neg[('Prozesskälte', 'ind')] = dict(zip(hours, hours_dummy_constant))\n",
    "hours_dict_neg[('Luftzerlegung', 'ind')] = dict(zip(hours, hours_dummy_constant))\n",
    "hours_dict_neg[('Kühlung (Lebensmittelindustrie)', 'ind')] = dict(zip(hours, hours_dummy_constant))\n",
    "hours_dict_neg[('Pumpenanwendungen', 'ind')] = dict(zip(hours, hours_dummy_morning_evening_reduced))\n",
    "hours_dict_neg[('Beleuchtung', 'ind')] = dict(zip(hours, hours_dummy_morning_evening_reduced))\n",
    "hours_dict_neg[('Druckluftanwendungen', 'ind')] = dict(zip(hours, hours_dummy_morning_evening_reduced))\n",
    "hours_dict_neg[('Belüftung', 'ind')] = dict(zip(hours, hours_dummy_morning_evening_reduced))\n",
    "\n",
    "# tcs\n",
    "hours_ava_light_tcs = [0.05, 0.05, 0.1, 0.1, 0.15, 0.2, \n",
    "                       0.2, 0, 0, 0.1, 0.1, 0.2, \n",
    "                       0.2, 0.2, 0.2, 0.2, 0.15, 0.1, \n",
    "                       0.1, 0.1, 0.1, 0.1, 0.05, 0.05]\n",
    "\n",
    "hours_dict_neg[('Prozesskälte', 'tcs')] = dict(zip(hours, hours_dummy_constant))\n",
    "hours_dict_neg[('Beleuchtung', 'tcs')] = dict(zip(hours, hours_dummy_morning_evening_reduced))\n",
    "hours_dict_neg[('Zerkleinerer', 'tcs')] = dict(zip(hours, hours_dummy_morning_evening_reduced))\n",
    "hours_dict_neg[('Prozesswärme', 'tcs')] = dict(zip(hours, hours_dummy_morning_evening_reduced))\n",
    "hours_dict_neg[('Abwasserbehandlung', 'tcs')] = dict(zip(hours, hours_dummy_morning_evening_reduced))\n",
    "\n",
    "# hoho\n",
    "hours_dict_neg[('Heizungsumwälzpumpen', 'hoho')] = dict(zip(hours, hours_dummy_morning_evening_reduced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign weekday profiles - positive\n",
    "days = range(0, 7)\n",
    "\n",
    "# dummies\n",
    "days_ava_dummy_week_constant = [1] * 7\n",
    "days_ava_dummy_weekend_slightly_reduced = [1] * 5 + [0.9, 0.75]\n",
    "days_ava_dummy_weekend_reduced = [1] * 5 + [0.5, 0.05]\n",
    "\n",
    "# ind\n",
    "days_dict_pos[('Klimakälte', 'ind')] = dict(zip(days, days_ava_dummy_weekend_slightly_reduced))\n",
    "days_dict_pos[('Prozesskälte', 'ind')] = dict(zip(days, days_ava_dummy_week_constant))\n",
    "days_dict_pos[('Luftzerlegung', 'ind')] = dict(zip(days, days_ava_dummy_week_constant))\n",
    "days_dict_pos[('Kühlung (Lebensmittelindustrie)', 'ind')] = dict(zip(days, days_ava_dummy_weekend_slightly_reduced))\n",
    "days_dict_pos[('Pumpenanwendungen', 'ind')] = dict(zip(days, days_ava_dummy_weekend_slightly_reduced))\n",
    "days_dict_pos[('Beleuchtung', 'ind')] = dict(zip(days, days_ava_dummy_weekend_slightly_reduced))\n",
    "days_dict_pos[('Druckluftanwendungen', 'ind')] = dict(zip(days, days_ava_dummy_weekend_slightly_reduced))\n",
    "days_dict_pos[('Belüftung', 'ind')] = dict(zip(days, days_ava_dummy_weekend_slightly_reduced))\n",
    "\n",
    "# tcs\n",
    "days_dict_pos[('Prozesskälte', 'tcs')] = dict(zip(days, days_ava_dummy_weekend_reduced))\n",
    "days_dict_pos[('Beleuchtung', 'tcs')] = dict(zip(days, days_ava_dummy_weekend_reduced))\n",
    "days_dict_pos[('Zerkleinerer', 'tcs')] = dict(zip(days, days_ava_dummy_weekend_slightly_reduced))\n",
    "days_dict_pos[('Prozesswärme', 'tcs')] = dict(zip(days, days_ava_dummy_weekend_reduced))\n",
    "days_dict_pos[('Abwasserbehandlung', 'tcs')] = dict(zip(days, days_ava_dummy_weekend_slightly_reduced))\n",
    "\n",
    "# tcs\n",
    "days_dict_pos[('Heizungsumwälzpumpen', 'hoho')] = dict(zip(days, days_ava_dummy_week_constant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign weekday profiles - negative\n",
    "days = range(0, 7)\n",
    "\n",
    "# dummies\n",
    "days_ava_dummy_week_constant = [1] * 7\n",
    "days_ava_dummy_weekend_slightly_reduced = [0.5] * 5 + [0.8, 1]\n",
    "days_ava_dummy_weekend_reduced = [0.2] * 5 + [0.5, 1]\n",
    "\n",
    "# ind\n",
    "days_dict_neg[('Klimakälte', 'ind')] = dict(zip(days, days_ava_dummy_weekend_slightly_reduced))\n",
    "days_dict_neg[('Prozesskälte', 'ind')] = dict(zip(days, days_ava_dummy_week_constant))\n",
    "days_dict_neg[('Luftzerlegung', 'ind')] = dict(zip(days, days_ava_dummy_week_constant))\n",
    "days_dict_neg[('Kühlung (Lebensmittelindustrie)', 'ind')] = dict(zip(days, days_ava_dummy_weekend_slightly_reduced))\n",
    "days_dict_neg[('Pumpenanwendungen', 'ind')] = dict(zip(days, days_ava_dummy_weekend_slightly_reduced))\n",
    "days_dict_neg[('Beleuchtung', 'ind')] = dict(zip(days, days_ava_dummy_weekend_slightly_reduced))\n",
    "days_dict_neg[('Druckluftanwendungen', 'ind')] = dict(zip(days, days_ava_dummy_weekend_slightly_reduced))\n",
    "days_dict_neg[('Belüftung', 'ind')] = dict(zip(days, days_ava_dummy_weekend_slightly_reduced))\n",
    "\n",
    "# tcs\n",
    "days_dict_neg[('Prozesskälte', 'tcs')] = dict(zip(days, days_ava_dummy_weekend_reduced))\n",
    "days_dict_neg[('Beleuchtung', 'tcs')] = dict(zip(days, days_ava_dummy_weekend_reduced))\n",
    "days_dict_neg[('Zerkleinerer', 'tcs')] = dict(zip(days, days_ava_dummy_weekend_slightly_reduced))\n",
    "days_dict_neg[('Prozesswärme', 'tcs')] = dict(zip(days, days_ava_dummy_weekend_reduced))\n",
    "days_dict_neg[('Abwasserbehandlung', 'tcs')] = dict(zip(days, days_ava_dummy_weekend_slightly_reduced))\n",
    "\n",
    "# tcs\n",
    "days_dict_neg[('Heizungsumwälzpumpen', 'hoho')] = dict(zip(days, days_ava_dummy_week_constant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign months profiles - positive\n",
    "months = range(1, 13)\n",
    "\n",
    "# dummies\n",
    "months_ava_dummy_constant = [1] * 12\n",
    "months_ava_dummy_cold_seasons = [1, 0.9, 0.8, 0.7, 0.5, 0.25, \n",
    "                                0.25, 0.25, 0.5, 0.7, 0.9, 1]\n",
    "months_ava_dummy_warm_seasons = [0] * 4 + [0.3, 0.7] + [1] * 2 + [0.6, 0.1] + [0] * 2\n",
    "month_ava_heating_seasons = [1, 0.8, 0.5, 0.1] + 5 * [0] + [0.2, 0.5, 1]\n",
    "\n",
    "# ind\n",
    "months_dict_pos[('Klimakälte', 'ind')] = dict(zip(months, months_ava_dummy_warm_seasons))\n",
    "months_dict_pos[('Prozesskälte', 'ind')] = dict(zip(months, months_ava_dummy_constant))\n",
    "months_dict_pos[('Luftzerlegung', 'ind')] = dict(zip(months, months_ava_dummy_constant))\n",
    "months_dict_pos[('Kühlung (Lebensmittelindustrie)', 'ind')] = dict(zip(months, months_ava_dummy_constant))\n",
    "months_dict_pos[('Pumpenanwendungen', 'ind')] = dict(zip(months, months_ava_dummy_constant))\n",
    "months_dict_pos[('Beleuchtung', 'ind')] = dict(zip(months, months_ava_dummy_cold_seasons))\n",
    "months_dict_pos[('Druckluftanwendungen', 'ind')] = dict(zip(months, months_ava_dummy_constant))\n",
    "months_dict_pos[('Belüftung', 'ind')] = dict(zip(months, months_ava_dummy_constant))\n",
    "\n",
    "# tcs\n",
    "months_dict_pos[('Prozesskälte', 'tcs')] = dict(zip(months, months_ava_dummy_constant))\n",
    "months_dict_pos[('Beleuchtung', 'tcs')] = dict(zip(months, months_ava_dummy_cold_seasons))\n",
    "months_dict_pos[('Zerkleinerer', 'tcs')] = dict(zip(months, months_ava_dummy_constant))\n",
    "months_dict_pos[('Prozesswärme', 'tcs')] = dict(zip(months, months_ava_dummy_constant))\n",
    "months_dict_pos[('Abwasserbehandlung', 'tcs')] = dict(zip(months, months_ava_dummy_constant))\n",
    "\n",
    "# tcs\n",
    "months_dict_pos[('Heizungsumwälzpumpen', 'hoho')] = dict(zip(months, month_ava_heating_seasons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign months profiles - negative\n",
    "months = range(1, 13)\n",
    "\n",
    "# dummies\n",
    "months_ava_dummy_constant = [1] * 12\n",
    "months_ava_dummy_cold_seasons = [0.2, 0.4, 0.5, 0.6, 1, 0.4, \n",
    "                                0.4, 0.5, 1, 0.7, 0.5, 0.2]\n",
    "months_ava_dummy_warm_seasons = [0] * 4 + [0.2, 0.4] + [0.2] * 2 + [1, 0.2] + [0] * 2\n",
    "month_ava_heating_seasons = [0.4, 0.7, 1, 0.4] + 5 * [0] + [0.4, 1, 0.4]\n",
    "\n",
    "# ind\n",
    "months_dict_neg[('Klimakälte', 'ind')] = dict(zip(months, months_ava_dummy_warm_seasons))\n",
    "months_dict_neg[('Prozesskälte', 'ind')] = dict(zip(months, months_ava_dummy_constant))\n",
    "months_dict_neg[('Luftzerlegung', 'ind')] = dict(zip(months, months_ava_dummy_constant))\n",
    "months_dict_neg[('Kühlung (Lebensmittelindustrie)', 'ind')] = dict(zip(months, months_ava_dummy_constant))\n",
    "months_dict_neg[('Pumpenanwendungen', 'ind')] = dict(zip(months, months_ava_dummy_constant))\n",
    "months_dict_neg[('Beleuchtung', 'ind')] = dict(zip(months, months_ava_dummy_cold_seasons))\n",
    "months_dict_neg[('Druckluftanwendungen', 'ind')] = dict(zip(months, months_ava_dummy_constant))\n",
    "months_dict_neg[('Belüftung', 'ind')] = dict(zip(months, months_ava_dummy_constant))\n",
    "\n",
    "# tcs\n",
    "months_dict_neg[('Prozesskälte', 'tcs')] = dict(zip(months, months_ava_dummy_constant))\n",
    "months_dict_neg[('Beleuchtung', 'tcs')] = dict(zip(months, months_ava_dummy_cold_seasons))\n",
    "months_dict_neg[('Zerkleinerer', 'tcs')] = dict(zip(months, months_ava_dummy_constant))\n",
    "months_dict_neg[('Prozesswärme', 'tcs')] = dict(zip(months, months_ava_dummy_constant))\n",
    "months_dict_neg[('Abwasserbehandlung', 'tcs')] = dict(zip(months, months_ava_dummy_constant))\n",
    "\n",
    "# tcs\n",
    "months_dict_neg[('Heizungsumwälzpumpen', 'hoho')] = dict(zip(months, month_ava_heating_seasons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing availability time series\n",
    "availability_ind_pos = assign_availability_remaining(\n",
    "    potential_ind_T_dict[\"SQ\"], availability_ind_pos,\n",
    "    synthetic_cols=[('Klimakälte', 'ind'), ('Prozesskälte', 'ind'), \n",
    "                    ('Luftzerlegung', 'ind'), \n",
    "                    ('Kühlung (Lebensmittelindustrie)', 'ind'), \n",
    "                    ('Pumpenanwendungen', 'ind'), ('Beleuchtung', 'ind'), \n",
    "                    ('Druckluftanwendungen', 'ind'), ('Belüftung', 'ind')],\n",
    "    hours_dict = hours_dict_pos,\n",
    "    days_dict = days_dict_pos,\n",
    "    months_dict = months_dict_pos)\n",
    "\n",
    "availability_ind_neg = assign_availability_remaining(\n",
    "    potential_ind_T_dict[\"SQ\"], availability_ind_neg,\n",
    "    synthetic_cols=[('Klimakälte', 'ind'), ('Prozesskälte', 'ind'), \n",
    "                    ('Luftzerlegung', 'ind'), \n",
    "                    ('Kühlung (Lebensmittelindustrie)', 'ind'), \n",
    "                    ('Pumpenanwendungen', 'ind'), ('Beleuchtung', 'ind'), \n",
    "                    ('Druckluftanwendungen', 'ind'), ('Belüftung', 'ind')],\n",
    "    hours_dict = hours_dict_neg,\n",
    "    days_dict = days_dict_neg,\n",
    "    months_dict = months_dict_neg)\n",
    "\n",
    "availability_tcs_pos = assign_availability_remaining(\n",
    "    potential_tcs_T_dict[\"SQ\"], availability_tcs_pos,\n",
    "    synthetic_cols=[('Prozesskälte', 'tcs'), ('Beleuchtung', 'tcs'), \n",
    "                    ('Zerkleinerer', 'tcs'), ('Prozesswärme', 'tcs'), \n",
    "                    ('Abwasserbehandlung', 'tcs')],\n",
    "    hours_dict = hours_dict_pos,\n",
    "    days_dict = days_dict_pos,\n",
    "    months_dict = months_dict_pos)\n",
    "\n",
    "availability_tcs_neg = assign_availability_remaining(\n",
    "    potential_tcs_T_dict[\"SQ\"], availability_tcs_neg,\n",
    "    synthetic_cols=[('Prozesskälte', 'tcs'), ('Beleuchtung', 'tcs'), \n",
    "                    ('Zerkleinerer', 'tcs'), ('Prozesswärme', 'tcs'), \n",
    "                    ('Abwasserbehandlung', 'tcs')],\n",
    "    hours_dict = hours_dict_neg,\n",
    "    days_dict = days_dict_neg,\n",
    "    months_dict = months_dict_neg)\n",
    "\n",
    "# No missing availability for hoho pos -> skip!\n",
    "\n",
    "availability_hoho_neg = assign_availability_remaining(\n",
    "    potential_hoho_T_dict[\"SQ\"], availability_hoho_neg,\n",
    "    synthetic_cols=[('Heizungsumwälzpumpen', 'hoho')],\n",
    "    hours_dict = hours_dict_neg,\n",
    "    days_dict = days_dict_neg,\n",
    "    months_dict = months_dict_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create load profiles per cluster\n",
    "Create load profiles for the demand response categories.\n",
    "In principle, two approaches could be used here:\n",
    "1. Most simple proxy: Use availability time series in positive direction. &rarr; drawback: very rough estimate\n",
    "2. More advanced: Use profiles per WZ as an intermediate product of the the demand regio disaggregator tool.\n",
    "\n",
    "Profiles are scaled with installed capacity in both cases. Simultaneity factors are introduced to account for the fact that not all demands happen simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation for load profile assessment\n",
    "* Provide availability time series with cluster information\n",
    "* Initialize empty dictionaries to store the time series\n",
    "* Determine which max. simultaneity factors to use &rarr; assumption is needed as long as there is no information on energy consumption or full load hours\n",
    "* Copy availability time series in order to be able to use it for both approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign cluster information\n",
    "availability_ind_pos.loc[\"cluster\"] = potential_ind_T_dict[\"SQ\"].loc[\"cluster\"]\n",
    "availability_ind_neg.loc[\"cluster\"] = potential_ind_T_dict[\"SQ\"].loc[\"cluster\"]\n",
    "availability_tcs_pos.loc[\"cluster\"] = potential_tcs_T_dict[\"SQ\"].loc[\"cluster\"]\n",
    "availability_tcs_neg.loc[\"cluster\"] = potential_tcs_T_dict[\"SQ\"].loc[\"cluster\"]\n",
    "availability_hoho_pos.loc[\"cluster\"] = potential_hoho_T_dict[\"SQ\"].loc[\"cluster\"]\n",
    "availability_hoho_neg.loc[\"cluster\"] = potential_hoho_T_dict[\"SQ\"].loc[\"cluster\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_ts_dict = {}\n",
    "tcs_ts_dict = {}\n",
    "hoho_ts_dict = {}\n",
    "overall_ts_dict = {}\n",
    "\n",
    "# determine maximum simultaneity factors per sector\n",
    "max_simultaneity_ind = 0.9\n",
    "max_simultaneity_tcs = 0.4\n",
    "max_simultaneity_hoho = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of availability time series\n",
    "load_profile_ind_pos = availability_ind_pos.copy()\n",
    "load_profile_tcs_pos = availability_tcs_pos.copy()\n",
    "load_profile_hoho_pos = availability_hoho_pos.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Use data from the demand regio disaggregator\n",
    "* Read in normalized profiles as output from the disaggregator tool\n",
    "* Map the demand response profiles with the WZs used in the disaggregator tool (hard-coded)\n",
    "* For the WZs which can be directly assigned, the respective demand pattern is used\n",
    "* For cross-cutting technologies no data exists, hence, the availability time series is used as a backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_ava_ts_for_profiles:\n",
    "    profiles_ind_normalized = pd.read_csv(\"profiles_industry_normalized.csv\", \n",
    "                                          index_col=0, parse_dates=True)\n",
    "    profiles_tcs_normalized = pd.read_csv(\"profiles_cts_normalized.csv\", \n",
    "                                          index_col=0, parse_dates=True)\n",
    "    profiles_hoho_normalized = pd.read_csv(\"profiles_households_normalized.csv\",\n",
    "                                           index_col=0, parse_dates=True)\n",
    "\n",
    "    WZ_mapping = pd.read_excel(\"remaining_categories_WZ_mapping.xlsx\", index_col=[0, 1])\n",
    "\n",
    "    profiles_ind_normalized.columns = profiles_ind_normalized.columns.astype(int)\n",
    "    profiles_tcs_normalized.columns = profiles_tcs_normalized.columns.astype(int)\n",
    "    profiles_hoho_normalized.columns = profiles_hoho_normalized.columns.astype(int)\n",
    "    \n",
    "    # Prepare WZ mapping\n",
    "    WZ_mapping.index.names = [\"Prozess\", \"help_sector\"]\n",
    "    WZ_mapping[\"sector\"] = np.where(WZ_mapping.index.get_level_values(1) == \"tcs+hoho\", \"hoho\",\n",
    "                                    WZ_mapping.index.get_level_values(1))\n",
    "    \n",
    "    WZ_mapping = WZ_mapping.reset_index(level=1)\n",
    "    WZ_mapping = WZ_mapping.set_index(\"sector\", append=True)\n",
    "\n",
    "    WZs_used = WZ_mapping[\"WZ\"].unique()\n",
    "    print(WZs_used)\n",
    "    \n",
    "    display(WZ_mapping.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_ava_ts_for_profiles:\n",
    "    \n",
    "    ind_ts_df = pd.DataFrame(index=profiles_ind_normalized.index,\n",
    "                         columns=potential_ind_T_dict[\"SQ\"].columns)\n",
    "    tcs_ts_df = pd.DataFrame(index=profiles_tcs_normalized.index,\n",
    "                             columns=potential_tcs_T_dict[\"SQ\"].columns)\n",
    "    hoho_ts_df = pd.DataFrame(index=profiles_hoho_normalized.index,\n",
    "                             columns=potential_hoho_T_dict[\"SQ\"].columns)\n",
    "\n",
    "    cols_used_ind = []\n",
    "    cols_used_tcs = []\n",
    "    cols_used_hoho = []\n",
    "\n",
    "    remaining_ts_cats = []\n",
    "    \n",
    "    for WZ in WZs_used:\n",
    "        ix = [list(el) for el in WZ_mapping[WZ_mapping[\"WZ\"] == WZ].index]\n",
    "        \n",
    "        if WZ in profiles_ind_normalized.columns:\n",
    "            ind_ts_df.loc[:,ix] = np.transpose([profiles_ind_normalized[WZ].values] * len(ix))\n",
    "            cols_used_ind.extend(ix)\n",
    "\n",
    "        elif WZ in profiles_tcs_normalized.columns:\n",
    "            tcs_ts_df.loc[:,ix] = np.transpose([profiles_tcs_normalized[WZ].values] * len(ix))\n",
    "            cols_used_tcs.extend(ix)\n",
    "\n",
    "        elif WZ in profiles_hoho_normalized.columns:\n",
    "            hoho_ts_df.loc[:,ix] = np.transpose([profiles_hoho_normalized[WZ].values] * len(ix))\n",
    "            cols_used_hoho.extend(ix)\n",
    "\n",
    "        else:\n",
    "            remaining_ts_cats.extend(ix)\n",
    "\n",
    "    overall_ts_df = pd.concat([ind_ts_df, \n",
    "                               tcs_ts_df,\n",
    "                               hoho_ts_df], axis=1)\n",
    "    \n",
    "    # Replace the default load time series with these values\n",
    "    load_profile_ind_pos.loc[:, cols_used_ind] = ind_ts_df.loc[:, cols_used_ind]\n",
    "    load_profile_tcs_pos.loc[:, cols_used_tcs] = tcs_ts_df.loc[:, cols_used_tcs]\n",
    "    load_profile_hoho_pos.loc[:, cols_used_hoho] = hoho_ts_df.loc[:, cols_used_hoho]\n",
    "    \n",
    "    # Add cluster info again\n",
    "    load_profile_ind_pos.loc[\"cluster\"] = availability_ind_pos.loc[\"cluster\"]\n",
    "    load_profile_tcs_pos.loc[\"cluster\"] = availability_tcs_pos.loc[\"cluster\"]\n",
    "    load_profile_hoho_pos.loc[\"cluster\"] = availability_hoho_pos.loc[\"cluster\"]\n",
    "    \n",
    "    # Determine cols that need a special treatment,\n",
    "    # i.e. columns for which the availability time series will be used\n",
    "    #remaining_ts_cats = list(set(potential_dict[\"SQ\"].index) - (set(overall_ts_df.columns)))\n",
    "    print(f\"Using availability time series information for categories: \\n{remaining_ts_cats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Use availability time series as simplest proxy.\n",
    "> _NOTE: If approach one was chosen, this will be run as well, but the data used will differ!_\n",
    "* Multiply availability time series with installed capacity and maximum simultaneity factor for the respective sector\n",
    "* Include cluster information for grouping.\n",
    "* Combine to an overall load profile time series dataset which will be grouped within the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    for col in cols:\n",
    "        # industry        \n",
    "        inst_cap_ind = potential_ind_T_dict[year].loc[col+\"_installed_cap\"]\n",
    "        ind_ts_dict[(year, col)] = pd.DataFrame(index=load_profile_ind_pos.index, \n",
    "                                                columns=load_profile_ind_pos.columns)\n",
    "        ind_ts_dict[(year, col)].iloc[:-1] = load_profile_ind_pos.iloc[:-1].mul(\n",
    "            inst_cap_ind).mul(max_simultaneity_ind)\n",
    "        ind_ts_dict[(year, col)].loc[\"cluster\"] = load_profile_ind_pos.loc[\"cluster\"]\n",
    "\n",
    "        # tcs\n",
    "        inst_cap_tcs = potential_tcs_T_dict[year].loc[col+\"_installed_cap\"]\n",
    "        tcs_ts_dict[(year, col)] = pd.DataFrame(index=load_profile_tcs_pos.index, \n",
    "                                                columns=load_profile_tcs_pos.columns)\n",
    "        tcs_ts_dict[(year, col)].iloc[:-1] = load_profile_tcs_pos.iloc[:-1].mul(\n",
    "            inst_cap_tcs).mul(max_simultaneity_tcs)\n",
    "        tcs_ts_dict[(year, col)].loc[\"cluster\"] = load_profile_tcs_pos.loc[\"cluster\"]\n",
    "\n",
    "        # hoho\n",
    "        inst_cap_hoho = potential_hoho_T_dict[year].loc[col+\"_installed_cap\"]\n",
    "        hoho_ts_dict[(year, col)] = pd.DataFrame(index=load_profile_hoho_pos.index, \n",
    "                                                 columns=load_profile_hoho_pos.columns)\n",
    "        hoho_ts_dict[(year, col)].iloc[:-1] = load_profile_hoho_pos.iloc[:-1].mul(\n",
    "            inst_cap_hoho).mul(max_simultaneity_hoho)\n",
    "        hoho_ts_dict[(year, col)].loc[\"cluster\"] = load_profile_hoho_pos.loc[\"cluster\"]\n",
    "\n",
    "        overall_ts_dict[(year, col)] = pd.concat([ind_ts_dict[(year, col)], \n",
    "                                                  tcs_ts_dict[(year, col)], \n",
    "                                                  hoho_ts_dict[(year, col)]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcs_ts_dict[(\"SQ\",\"50%\")].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize load profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_ts_dict[(\"SQ\", \"50%\")].iloc[:-1].plot()\n",
    "plt.ylim([0,2500])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcs_ts_dict[(\"SQ\", \"50%\")].iloc[:-1].plot()\n",
    "plt.ylim([0,2500])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoho_ts_dict[(\"SQ\", \"50%\")].iloc[:-1].plot()\n",
    "plt.ylim([0,10000])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##  Create availability time series per cluster\n",
    "* Calculate weighted averages within clusters.\n",
    "* Save the results, i.e. availability time series per cluster, to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_dict = {}\n",
    "\n",
    "for year in years:\n",
    "\n",
    "    sector_dict[year] = {\"ind\": (availability_ind_pos, availability_ind_neg, potential_ind_T_dict[year]),\n",
    "                       \"tcs\": (availability_tcs_pos, availability_tcs_neg, potential_tcs_T_dict[year]),\n",
    "                       \"hoho\": (availability_hoho_pos, availability_hoho_neg, potential_hoho_T_dict[year])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of availability factors per cluster**:\n",
    "* Maximum availability per cluster does not necessarily have to be 1.\n",
    "* A value of 0.88 implies that at maximum 88% of the overall cluster capacity are available at the same time. &rarr; Similar interpretation than simultaneity factor.\n",
    "* Nevertheless, a rescaling is done here for the sake of easier interpretation. &rarr; I.e. maximum values will be 1.0 and cluster capacity is adapted accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(potential_T_dict[\"SQ\"].loc[\"cluster\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract info on combined clusters for tcs and hoho separately in order not to overwrite it\n",
    "tcs_hoho_clusters = potential_dict[\"SQ\"].loc[potential_dict[\"SQ\"].index.get_level_values(1) == \"tcs+hoho\", \n",
    "                                             \"cluster\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TODO: Check assignments here! Seems a bit buggy / inconvenient!_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pot_cols = [\"potential_pos_overall\", \"potential_neg_overall\", \"installed_cap\"]\n",
    "cols_of_interest = [i+\"_\"+j for i in cols for j in pot_cols]\n",
    "\n",
    "# Use dicts and DataFrames to store overall cluster potential and availability time series per cluster\n",
    "cluster_overall_pot_dict = {}\n",
    "cluster_overall_ts_dict = {}\n",
    "availability_clusters = pd.DataFrame(index=availability_ind_pos.index).drop(\"cluster\")\n",
    "\n",
    "# Create availability time series within clusters by calculating weighted averages\n",
    "for year in years:\n",
    "    \n",
    "    cluster_overall_pot_df = pd.DataFrame(columns=cols_of_interest)\n",
    "    load_timeseries = pd.DataFrame(index=overall_ts_dict[(\"SQ\",\"50%\")].iloc[:-1].index)\n",
    "    \n",
    "    for el in np.sort(potential_T_dict[\"SQ\"].loc[\"cluster\"].unique()):\n",
    "        \n",
    "        to_concat = []\n",
    "        cluster_data = {}\n",
    "        \n",
    "        for col in cols:\n",
    "            sector = potential_T_dict[year].loc[:,potential_T_dict[year].loc[\n",
    "                \"cluster\"] == el].columns.get_level_values(1)[0]\n",
    "            \n",
    "            # introduce shortcuts for readability\n",
    "            org_ava_pos = sector_dict[year][sector][0]\n",
    "            org_ava_neg = sector_dict[year][sector][1]\n",
    "            pots_T = sector_dict[year][sector][2]\n",
    "            \n",
    "            # Calculate a weighted average for positive potentials (weights: maximum potential information)\n",
    "            ava_pos = org_ava_pos.loc[:,org_ava_pos.loc[\"cluster\"] == el].drop(\"cluster\")    \n",
    "            pot_pos = pots_T.loc[col+\"_potential_pos_overall\", pots_T.loc[\"cluster\"] == el]\n",
    "            overall_pot_pos = pot_pos.sum()\n",
    "            \n",
    "            if overall_pot_pos != 0:\n",
    "                ava_pos[col+\"_pos_cluster_\"+str(el)] = ava_pos.mul(pot_pos).div(overall_pot_pos).sum(axis=1)\n",
    "\n",
    "                # Do rescaling: Adjust (reduce) potential information if necessary \n",
    "                # and scale max value of availability time series to 1            \n",
    "                max_pot_pos = ava_pos[col+\"_pos_cluster_\"+str(el)].max() * overall_pot_pos\n",
    "                ava_pos[col+\"_pos_cluster_\"+str(el)] = \\\n",
    "                    ava_pos[col+\"_pos_cluster_\"+str(el)].div(ava_pos[col+\"_pos_cluster_\"+str(el)].max())\n",
    "            \n",
    "            # Calculate a weighted average for negative potentials (weights: potential information)\n",
    "            ava_neg = org_ava_neg.loc[:,org_ava_neg.loc[\"cluster\"] == el].drop(\"cluster\")    \n",
    "            pot_neg = pots_T.loc[col+\"_potential_neg_overall\", pots_T.loc[\"cluster\"] == el]\n",
    "            overall_pot_neg = pot_neg.sum()\n",
    "            \n",
    "            if overall_pot_neg != 0:\n",
    "                ava_neg[col+\"_neg_cluster_\"+str(el)] = ava_neg.mul(pot_neg).div(overall_pot_neg).sum(axis=1)\n",
    "\n",
    "                # Do rescaling: Adjust (reduce) potential information if necessary \n",
    "                # and scale max value of availability time series to 1\n",
    "                max_pot_neg = ava_neg[col+\"_neg_cluster_\"+str(el)].max() * overall_pot_neg\n",
    "                ava_neg[col+\"_neg_cluster_\"+str(el)] = \\\n",
    "                    ava_neg[col+\"_neg_cluster_\"+str(el)].div(ava_neg[col+\"_neg_cluster_\"+str(el)].max())\n",
    "\n",
    "            if el not in tcs_hoho_clusters:\n",
    "                cluster_label = sector+\"_cluster-\"+str(int(el))\n",
    "            else:\n",
    "                cluster_label = \"tcs+hoho_cluster-\"+str(int(el))\n",
    "\n",
    "            # show exemplarily availability time series for clusters\n",
    "            if year == \"SQ\":\n",
    "                availability_clusters[col+\"_\"+cluster_label+\"_pos\"] = ava_pos[col+\"_pos_cluster_\"+str(el)]\n",
    "                availability_clusters[col+\"_\"+cluster_label+\"_neg\"] = ava_neg[col+\"_neg_cluster_\"+str(el)]\n",
    "                availability_clusters = availability_clusters.round(4)\n",
    "            \n",
    "                if col == \"50%\":\n",
    "                    # Show a sample week for the clusters (SQ, positive potentials and median only)\n",
    "                    fig, ax = plt.subplots(figsize=(10,5))\n",
    "                    _ = ava_pos.iloc[:168,:-1].plot(ax=ax)\n",
    "                    _ = ava_pos.iloc[:168,-1:].plot(ax=ax, linewidth=5)\n",
    "                    _ = plt.title(\"Cluster \"+str(el))\n",
    "                    _ = plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "                    plt.show()\n",
    "                    fig.savefig(\"cluster_\"+str(el)+\".png\")\n",
    "                    plt.close()\n",
    "            \n",
    "            # Save potential outputs\n",
    "            cluster_data[col+\"_\"+pot_cols[0]] = max_pot_pos\n",
    "            cluster_data[col+\"_\"+pot_cols[1]] = max_pot_neg\n",
    "            \n",
    "            # Assign load profiles and store them in a dict\n",
    "            load_timeseries[col+\"_\"+cluster_label] = overall_ts_dict[(year, col)].loc[\n",
    "                :,overall_ts_dict[(year, col)].loc[\"cluster\"] == el].sum(axis=1).drop(\"cluster\")\n",
    "            \n",
    "            max_cap = load_timeseries[col+\"_\"+cluster_label].max()\n",
    "            \n",
    "            cluster_data[col+\"_\"+pot_cols[2]] = max_cap\n",
    "            \n",
    "        # Combine potential outputs and store them in dict of DataFrames\n",
    "        cluster_overall_pot_df.loc[cluster_label, [i+\"_\"+j for i in cols \n",
    "                                                           for j in pot_cols]] = cluster_data\n",
    "\n",
    "        cluster_overall_pot_dict[year] = cluster_overall_pot_df\n",
    "    \n",
    "        cluster_overall_ts_dict[year] = load_timeseries\n",
    "\n",
    "display(availability_clusters.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce some fixes:\n",
    "* Normalize load profiles again in order to be able to use them combined with maximum capacity demand\n",
    "* Rename installed\\_cap to max\\_cap in order to prevent misinterpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize load profiles (for usage in oemof)\n",
    "# Rename column to max cap in order to be able to distinct it from installed capacity\n",
    "for year in years:\n",
    "    cluster_overall_ts_dict[year] = cluster_overall_ts_dict[year].div(cluster_overall_ts_dict[year].max())\n",
    "    for col in cols:\n",
    "        cluster_overall_pot_dict[year].rename({col + \"_installed_cap\": col + \"_max_cap\" for col in cols},\n",
    "                                      axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_overall_pot_dict[\"SQ\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_overall_pot_dict[\"2050\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_overall_ts_dict[\"2050\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = cluster_overall_pot_dict[\"SQ\"].index.values\n",
    "cluster_ts_by_cols = {}\n",
    "ava_pos_ts_by_cols = {}\n",
    "ava_neg_ts_by_cols = {}\n",
    "\n",
    "# Split timeseries into subsets\n",
    "for col in cols:\n",
    "    for year in years:\n",
    "        ava_cols = [i+\"_\"+j for i in [col] for j in cluster_labels]\n",
    "        ava_cols_pos = [i+\"_\"+j+\"_pos\" for i in [col] for j in cluster_labels]\n",
    "        ava_cols_neg = [i+\"_\"+j+\"_neg\" for i in [col] for j in cluster_labels]\n",
    "\n",
    "        cluster_ts_by_cols[col+\"_\"+year] = cluster_overall_ts_dict[year][ava_cols]\n",
    "        ava_pos_ts_by_cols[col+\"_\"+year] = availability_clusters[ava_cols_pos]\n",
    "        ava_neg_ts_by_cols[col+\"_\"+year] = availability_clusters[ava_cols_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ts_by_cols[\"25%_SQ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_outputs:\n",
    "    availability_clusters.to_csv(path_folder_availability+filename_availability_out, sep=\";\", decimal=\",\")\n",
    "    write_multiple_sheets(cluster_ts_by_cols, path_folder_parameterization, \n",
    "                          filename_load_profiles_out+\".xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group the data within the clusters and write outputs\n",
    "Determine, how the grouping will take place and which aggregation rules to apply for a parameter:\n",
    "* *grouping_cols*: Columns to group by (cluster numbers)\n",
    "* *mean_cols*: Columns for which the weighted average is used\n",
    "* *sum_cols*: Columns for which entries are summed up\n",
    "\n",
    "The average positive potential is used for calculating weigthed averages.<br>\n",
    "> _Note: Again, there are possible alternatives here:_\n",
    "> * _Use the installed capacity &rarr; drawback: missing for most demand response categories._\n",
    "> * _Use positive or negative capacity dependent on parameter. &rarr; drawback: may lead to inconsistencies due to differing weights._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parameters of interest for further model-based analyses\n",
    "params_of_interest_all = params_tbu + params_remaining\n",
    "params_of_interest_all.remove(\"year_key\")\n",
    "params_of_interest = set()\n",
    "for el in params_of_interest_all:\n",
    "    params_of_interest.add(el[4:])\n",
    "params_of_interest = list(params_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "grouping_cols = [\"cluster\"]\n",
    "other_cols = []\n",
    "\n",
    "mean_list = []\n",
    "sum_list = []\n",
    "for k, v in parameters_groups_dict.items():\n",
    "    if k not in params_of_interest:\n",
    "        continue\n",
    "    if v[0] == \"mean\":\n",
    "        mean_list.append(k)\n",
    "    elif v[0] == \"sum\":\n",
    "        sum_list.append(k)\n",
    "        \n",
    "mean_cols = create_parameter_combinations(mean_list, cols)\n",
    "sum_cols = create_parameter_combinations(sum_list, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming of cluster identifiers\n",
    "name_dict = {\"ind_cluster-2112\": \"ind_cluster-1\",\n",
    "             \"ind_cluster-2113\": \"ind_cluster-2\",\n",
    "             \"tcs_cluster-12\": \"tcs_cluster\",\n",
    "             \"hoho_cluster-2\": \"hoho_cluster\",\n",
    "             \"tcs+hoho_cluster-112\": \"tcs_hoho_cluster\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_of_rating(number):\n",
    "    \"\"\"Round a number to the closest quarter integer. \"\"\"\n",
    "    return round(number * 4) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the parameters for the clusters\n",
    "ind_dict = {}\n",
    "tcs_dict = {}\n",
    "hoho_dict = {}\n",
    "tcs_hoho_dict = {}\n",
    "overall_dict = {}\n",
    "\n",
    "pot_cols = [\"potential_pos_overall\", \"potential_neg_overall\", \"max_cap\"]\n",
    "dur_cols = [\"interference_duration_neg\", \"interference_duration_pos\", \n",
    "            \"regeneration_duration\", \"shifting_duration\"]\n",
    "cost_cols = [\"fixed_costs\", \"variable_costs\", \"specific_investments\"]\n",
    "other_cols = [\"maximum_activations_year\", \"installed_cap\", \"max_cap\",\n",
    "              \"potential_neg_overall\", \"potential_pos_overall\"]\n",
    "\n",
    "for col in cols:\n",
    "    for year in years:\n",
    "        ind_dict[col+\"_\"+year] = group_potential(potential_dict[year][potential_dict[year].index.get_level_values(1) == \"ind\"], \n",
    "                                                 grouping_cols, weight=col+\"_potential_pos_overall\", mean_cols=mean_cols, \n",
    "                                                 sum_cols=sum_cols, other_cols=other_cols,\n",
    "                                                 sector=\"ind\", drop=[el for el in cols if el != col]).round(2)\n",
    "        \n",
    "        tcs_dict[col+\"_\"+year] = group_potential(potential_dict[year][potential_dict[year].index.get_level_values(1) == \"tcs\"], \n",
    "                                                 grouping_cols, weight=col+\"_potential_pos_overall\", mean_cols=mean_cols, \n",
    "                                                 sum_cols=sum_cols, other_cols=other_cols,\n",
    "                                                 sector=\"tcs\", drop=[el for el in cols if el != col]).round(2)\n",
    "        \n",
    "        hoho_dict[col+\"_\"+year] = group_potential(potential_dict[year][potential_dict[year].index.get_level_values(1) == \"hoho\"], \n",
    "                                                  grouping_cols, weight=col+\"_potential_pos_overall\", mean_cols=mean_cols, \n",
    "                                                  sum_cols=sum_cols,  other_cols=other_cols,\n",
    "                                                  sector=\"hoho\", drop=[el for el in cols if el != col]).round(2)\n",
    "        \n",
    "        tcs_hoho_dict[col+\"_\"+year] = group_potential(potential_dict[year][potential_dict[year].index.get_level_values(1) == \"tcs+hoho\"], \n",
    "                                                      grouping_cols, weight=col+\"_potential_pos_overall\", mean_cols=mean_cols, \n",
    "                                                      sum_cols=sum_cols,  other_cols=other_cols,\n",
    "                                                      sector=\"tcs+hoho\", drop=[el for el in cols if el != col]).round(2)\n",
    "        \n",
    "        cols_potentials = [i+\"_\"+j for i in [col] for j in pot_cols]\n",
    "\n",
    "        # Update potential data with availability information from above\n",
    "        if adjust_potentials:\n",
    "            ind_dict[col+\"_\"+year][cols_potentials] = \\\n",
    "                cluster_overall_pot_dict[year][cols_potentials].astype(float).round(2)\n",
    "            tcs_dict[col+\"_\"+year][cols_potentials] = \\\n",
    "                cluster_overall_pot_dict[year][cols_potentials].astype(float).round(2)\n",
    "            hoho_dict[col+\"_\"+year][cols_potentials] = \\\n",
    "                cluster_overall_pot_dict[year][cols_potentials].astype(float).round(2)\n",
    "            tcs_hoho_dict[col+\"_\"+year][cols_potentials] = \\\n",
    "                cluster_overall_pot_dict[year][cols_potentials].astype(float).round(2)\n",
    "        \n",
    "        # Add country and bus information (needed in power market model)\n",
    "        ind_dict[col+\"_\"+year][col+\"_country\"] = \"DE\"\n",
    "        tcs_dict[col+\"_\"+year][col+\"_country\"] = \"DE\"\n",
    "        hoho_dict[col+\"_\"+year][col+\"_country\"] = \"DE\"\n",
    "        tcs_hoho_dict[col+\"_\"+year][col+\"_country\"] = \"DE\"\n",
    "        \n",
    "        ind_dict[col+\"_\"+year][col+\"_from\"] = \"DE_bus_el\"\n",
    "        tcs_dict[col+\"_\"+year][col+\"_from\"] = \"DE_bus_el\"\n",
    "        hoho_dict[col+\"_\"+year][col+\"_from\"] = \"DE_bus_el\"\n",
    "        tcs_hoho_dict[col+\"_\"+year][col+\"_from\"] = \"DE_bus_el\"\n",
    "        \n",
    "        # Combine to dict holding all parameter data for the clusters\n",
    "        overall_dict[col+\"_\"+year] = pd.concat([ind_dict[col+\"_\"+year], \n",
    "                                                tcs_dict[col+\"_\"+year], \n",
    "                                                hoho_dict[col+\"_\"+year], \n",
    "                                                tcs_hoho_dict[col+\"_\"+year]])\n",
    "        \n",
    "        # Adjust rounding: costs to 1 digit; durations to nearest quarter integer; other params to 0 digits\n",
    "        cols_duration = [i+\"_\"+j for i in [col] for j in dur_cols]\n",
    "        cols_costs = [i+\"_\"+j for i in [col] for j in cost_cols]\n",
    "        cols_other = [i+\"_\"+j for i in [col] for j in other_cols]\n",
    "        \n",
    "        overall_dict[col+\"_\"+year][cols_duration] = overall_dict[col+\"_\"+year][cols_duration].apply(\n",
    "            round_of_rating)\n",
    "        overall_dict[col+\"_\"+year][cols_costs] = overall_dict[col+\"_\"+year][cols_costs].round(1)\n",
    "        # Replace zero cost values which might occur in rounding\n",
    "        overall_dict[col+\"_\"+year][cols_costs] = overall_dict[col+\"_\"+year][cols_costs].replace({0.0:0.01})\n",
    "        overall_dict[col+\"_\"+year][cols_other] = overall_dict[col+\"_\"+year][cols_other].round(0)\n",
    "        \n",
    "        # Do some renaming\n",
    "        overall_dict[col+\"_\"+year].rename(name_dict, inplace=True)\n",
    "        overall_dict[col+\"_\"+year].columns = overall_dict[col+\"_\"+year].columns.str[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_dict[\"50%_2050\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Info on remaining categories is optionally stored in order to be able to match it with availability time series\n",
    "# resp. to assign similar availability time series when data is missing.\n",
    "if write_categories:\n",
    "    potential_dict[\"SQ\"].to_excel(\"remaining_categories.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Write the parameter outputs to Excel\n",
    "if write_outputs:\n",
    "    write_multiple_sheets(ind_dict, path_folder_parameterization, filename_out+\"_ind.xlsx\")\n",
    "    write_multiple_sheets(tcs_dict, path_folder_parameterization, filename_out+\"_tcs.xlsx\")\n",
    "    write_multiple_sheets(hoho_dict, path_folder_parameterization, filename_out+\"_hoho.xlsx\")\n",
    "    write_multiple_sheets(tcs_hoho_dict, path_folder_parameterization, filename_out+\"_tcs_hoho.xlsx\") \n",
    "    write_multiple_sheets(overall_dict, path_folder_parameterization, filename_out+\"_overall.xlsx\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write outputs\n",
    "Write outputs needed for the power market model runs separately to csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some adjustments:\n",
    "* Simple column naming adjustment\n",
    "* Timeseries adjustments / harmonization:\n",
    "> __*NOTE*__: _2017 is used as a simulation year for the power market model._\n",
    "> * _Availability time series were (mostly) derived for 2012._\n",
    "> * _Since 2017 and 2012 both started with a Sunday, no shifts of weekdays is necessary._\n",
    "> * _Only the last day of 2012 is cut here since 2012 was a leap year._\n",
    "> * _At the end, this only is a very rough estimate / first guess which **should be improved**._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some column naming adjustment\n",
    "cases = [\"25%_SQ\", \"50%_SQ\", \"75%_SQ\"]\n",
    "\n",
    "for case in cases:\n",
    "    cluster_ts_by_cols[case].columns = cluster_ts_by_cols[case].columns.str[4:]\n",
    "    ava_pos_ts_by_cols[case].columns = ava_pos_ts_by_cols[case].columns.str[4:-4]\n",
    "    ava_neg_ts_by_cols[case].columns = ava_neg_ts_by_cols[case].columns.str[4:-4]\n",
    "\n",
    "    cluster_ts_by_cols[case].rename(name_dict, axis=1, inplace=True)\n",
    "    ava_pos_ts_by_cols[case].rename(name_dict, axis=1, inplace=True)\n",
    "    ava_neg_ts_by_cols[case].rename(name_dict, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do time series harmonization thing\n",
    "new_timeindex = pd.date_range(start=\"2017-01-01 00:00\", periods=8784, freq=\"H\")\n",
    "\n",
    "for case in cases:\n",
    "    cluster_ts_by_cols[case][\"new_timeindex\"] = new_timeindex\n",
    "    ava_pos_ts_by_cols[case][\"new_timeindex\"] = new_timeindex\n",
    "    ava_neg_ts_by_cols[case][\"new_timeindex\"] = new_timeindex\n",
    "    \n",
    "    cluster_ts_by_cols[case] = cluster_ts_by_cols[case].set_index(\"new_timeindex\", drop=True).iloc[:8760].round(4)\n",
    "    ava_pos_ts_by_cols[case] = ava_pos_ts_by_cols[case].set_index(\"new_timeindex\", drop=True).iloc[:8760].round(4)\n",
    "    ava_neg_ts_by_cols[case] = ava_neg_ts_by_cols[case].set_index(\"new_timeindex\", drop=True).iloc[:8760].round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ts_by_cols[\"25%_SQ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for case in cases:\n",
    "    # Node information\n",
    "    overall_dict[case].to_csv(path_folder_parameterization+\"sinks_demand_response_el_\"+case[:2]+\".csv\")\n",
    "    # Load profile information\n",
    "    cluster_ts_by_cols[case].to_csv(path_folder_parameterization+\"sinks_demand_response_el_ts_\"+case[:2]+\".csv\")\n",
    "    # Availability information\n",
    "    ava_pos_ts_by_cols[case].to_csv(\n",
    "        path_folder_parameterization+\"sinks_demand_response_el_ava_pos_ts_\"+case[:2]+\".csv\")\n",
    "    ava_neg_ts_by_cols[case].to_csv(\n",
    "        path_folder_parameterization+\"sinks_demand_response_el_ava_neg_ts_\"+case[:2]+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "Gils, Hans Christian (2015): Balancing of Intermittent Renewable Power Generation by Demand Response and Thermal Energy Storage. Dissertation. Universität Stuttgart, Stuttgart.\n",
    "\n",
    "Kochems, Johannes (2020): Lastflexibilisierungspotenziale in Deutschland - Bestandsaufnahme und Entwicklungsprojektionen. Langfassung. In: IEE TU Graz (Hg.): EnInnov 2020 - 16. Symposium Energieinnovation. Energy for Future - Wege zur Klimaneutralität. Graz, 12.-14.02, https://www.tugraz.at/fileadmin/user_upload/tugrazExternal/4778f047-2e50-4e9e-b72d-e5af373f95a4/files/lf/Session_E5/553_LF_Kochems.pdf, accessed 11.05.2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "579.933px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "608.85px",
    "left": "2182.67px",
    "right": "20px",
    "top": "128px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
